{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6884061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Any, Set\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ContentFilter:\n",
    "    \"\"\"Content filter for company-specific news article relevance and deduplication\"\"\"\n",
    "    \n",
    "    def __init__(self, relevance_threshold=0.2, max_articles_per_company=50, max_days_old=7):\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.max_articles_per_company = max_articles_per_company\n",
    "        self.max_days_old = max_days_old\n",
    "        \n",
    "        # Financial keywords for relevance scoring\n",
    "        self.financial_keywords = self._load_financial_keywords()\n",
    "        \n",
    "        # Stop words for better matching\n",
    "        self.stop_words = self._load_stop_words()\n",
    "        \n",
    "        logger.info(f\"ContentFilter initialized: threshold={relevance_threshold}, max_articles={max_articles_per_company}\")\n",
    "    \n",
    "    def _load_financial_keywords(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Load financial keywords for relevance scoring\n",
    "        Categorized by importance/weight\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'high_weight': [\n",
    "                'earnings', 'results', 'profit', 'loss', 'revenue', 'turnover',\n",
    "                'quarterly', 'annual', 'financial', 'performance', 'growth',\n",
    "                'dividend', 'bonus', 'split', 'ipo', 'listing', 'merger',\n",
    "                'acquisition', 'takeover', 'buyback', 'rights issue'\n",
    "            ],\n",
    "            'medium_weight': [\n",
    "                'shares', 'stock', 'market', 'trading', 'volume', 'price',\n",
    "                'valuation', 'investment', 'investor', 'fund', 'portfolio',\n",
    "                'analyst', 'rating', 'target', 'recommendation', 'upgrade',\n",
    "                'downgrade', 'outlook', 'forecast', 'guidance', 'expansion'\n",
    "            ],\n",
    "            'low_weight': [\n",
    "                'company', 'business', 'corporate', 'management', 'board',\n",
    "                'director', 'ceo', 'chairman', 'announcement', 'news',\n",
    "                'update', 'launch', 'product', 'service', 'contract',\n",
    "                'agreement', 'deal', 'partnership', 'collaboration'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def _load_stop_words(self) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Load common stop words to ignore during matching\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be',\n",
    "            'have', 'has', 'had', 'will', 'would', 'could', 'should',\n",
    "            'a', 'an', 'this', 'that', 'these', 'those', 'ltd', 'limited',\n",
    "            'pvt', 'private', 'company', 'corp', 'corporation', 'inc'\n",
    "        }\n",
    "    \n",
    "    def filter_company_articles(self, articles: List[Dict[str, Any]], company_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Main filtering function: Filter articles for company relevance\n",
    "        Returns list of relevant articles sorted by relevance and recency\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not articles or not company_data:\n",
    "                logger.warning(\"Empty articles or company_data provided\")\n",
    "                return []\n",
    "            \n",
    "            logger.info(f\"Filtering {len(articles)} articles for {company_data.get('company_name', 'Unknown')}\")\n",
    "            \n",
    "            # Extract company identifiers\n",
    "            company_identifiers = self._extract_company_identifiers(company_data)\n",
    "            \n",
    "            # Filter for relevance\n",
    "            relevant_articles = []\n",
    "            for article in articles:\n",
    "                relevance_score = self.check_article_relevance(article, company_data.get('company_name', ''), \n",
    "                                                             company_data.get('symbol', ''))\n",
    "                \n",
    "                if relevance_score >= self.relevance_threshold:\n",
    "                    article['relevance_score'] = relevance_score\n",
    "                    article['company_identifiers_found'] = self._find_matching_identifiers(article, company_identifiers)\n",
    "                    relevant_articles.append(article)\n",
    "            \n",
    "            logger.info(f\"Found {len(relevant_articles)} relevant articles (threshold: {self.relevance_threshold})\")\n",
    "            \n",
    "            if not relevant_articles:\n",
    "                return []\n",
    "            \n",
    "            # Remove duplicates\n",
    "            deduplicated_articles = self.deduplicate_articles(relevant_articles)\n",
    "            \n",
    "            # Filter by date\n",
    "            date_filtered_articles = self._filter_by_date(deduplicated_articles)\n",
    "            \n",
    "            # Sort by relevance and recency\n",
    "            sorted_articles = self.sort_by_recency(date_filtered_articles)\n",
    "            \n",
    "            # Limit number of articles\n",
    "            final_articles = sorted_articles[:self.max_articles_per_company]\n",
    "            \n",
    "            logger.info(f\"Final filtered result: {len(final_articles)} articles for {company_data.get('symbol', 'Unknown')}\")\n",
    "            \n",
    "            return final_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in filter_company_articles: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def check_article_relevance(self, article: Dict[str, Any], company_name: str, ticker: str) -> float:\n",
    "        \"\"\"\n",
    "        Check article relevance using basic string matching\n",
    "        Returns relevance score between 0.0 and 1.0\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not article or not company_name:\n",
    "                return 0.0\n",
    "            \n",
    "            # Get article text content\n",
    "            title = self._clean_text(article.get('title', ''))\n",
    "            description = self._clean_text(article.get('description', ''))\n",
    "            \n",
    "            # Combine all text for analysis\n",
    "            article_text = f\"{title} {description}\".lower()\n",
    "            \n",
    "            if not article_text.strip():\n",
    "                return 0.0\n",
    "            \n",
    "            relevance_score = 0.0\n",
    "            \n",
    "            # 1. Direct symbol/ticker matching (highest weight)\n",
    "            if ticker and ticker.lower() in article_text:\n",
    "                relevance_score += 0.4\n",
    "                logger.debug(f\"Direct ticker match found: {ticker}\")\n",
    "            \n",
    "            # 2. Company name matching\n",
    "            company_score = self._calculate_company_name_score(article_text, company_name)\n",
    "            relevance_score += company_score * 0.3\n",
    "            \n",
    "            # 3. Financial keywords matching\n",
    "            keyword_score = self._calculate_keyword_score(article_text)\n",
    "            relevance_score += keyword_score * 0.2\n",
    "            \n",
    "            # 4. Context relevance (title vs description weight)\n",
    "            context_score = self._calculate_context_score(title, description, company_name, ticker)\n",
    "            relevance_score += context_score * 0.1\n",
    "            \n",
    "            # Normalize score to [0, 1]\n",
    "            relevance_score = min(relevance_score, 1.0)\n",
    "            \n",
    "            logger.debug(f\"Article relevance score: {relevance_score:.3f} for '{title[:50]}...'\")\n",
    "            \n",
    "            return relevance_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating article relevance: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _extract_company_identifiers(self, company_data: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Extract all possible identifiers for the company\n",
    "        \"\"\"\n",
    "        identifiers = {\n",
    "            'symbols': [],\n",
    "            'company_names': [],\n",
    "            'search_terms': []\n",
    "        }\n",
    "        \n",
    "        # Extract symbols\n",
    "        symbol = company_data.get('symbol', '').strip()\n",
    "        nse_symbol = company_data.get('nse_symbol', '').strip()\n",
    "        \n",
    "        if symbol:\n",
    "            identifiers['symbols'].append(symbol.upper())\n",
    "        if nse_symbol and nse_symbol != symbol:\n",
    "            identifiers['symbols'].append(nse_symbol.upper())\n",
    "        \n",
    "        # Extract company names\n",
    "        company_name = company_data.get('company_name', '').strip()\n",
    "        if company_name:\n",
    "            identifiers['company_names'].append(company_name)\n",
    "            \n",
    "            # Add cleaned version without suffixes\n",
    "            cleaned_name = self._clean_company_name(company_name)\n",
    "            if cleaned_name != company_name:\n",
    "                identifiers['company_names'].append(cleaned_name)\n",
    "        \n",
    "        # Extract search terms if available\n",
    "        search_terms = company_data.get('search_terms', [])\n",
    "        if isinstance(search_terms, list):\n",
    "            identifiers['search_terms'].extend(search_terms)\n",
    "        \n",
    "        return identifiers\n",
    "    \n",
    "    def _clean_company_name(self, company_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean company name by removing common suffixes\n",
    "        \"\"\"\n",
    "        if not company_name:\n",
    "            return \"\"\n",
    "        \n",
    "        clean_name = company_name.strip()\n",
    "        \n",
    "        # Remove common suffixes\n",
    "        suffixes = [\n",
    "            ' Ltd', ' Limited', ' Pvt', ' Private', ' Company',\n",
    "            ' Corp', ' Corporation', ' Inc', ' Incorporated'\n",
    "        ]\n",
    "        \n",
    "        for suffix in suffixes:\n",
    "            if clean_name.endswith(suffix):\n",
    "                clean_name = clean_name[:-len(suffix)].strip()\n",
    "                break\n",
    "        \n",
    "        return clean_name\n",
    "    \n",
    "    def _find_matching_identifiers(self, article: Dict[str, Any], identifiers: Dict[str, List[str]]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Find which company identifiers are present in the article\n",
    "        \"\"\"\n",
    "        article_text = f\"{article.get('title', '')} {article.get('description', '')}\".lower()\n",
    "        found_identifiers = []\n",
    "        \n",
    "        # Check symbols\n",
    "        for symbol in identifiers.get('symbols', []):\n",
    "            if symbol.lower() in article_text:\n",
    "                found_identifiers.append(f\"symbol:{symbol}\")\n",
    "        \n",
    "        # Check company names\n",
    "        for name in identifiers.get('company_names', []):\n",
    "            if name.lower() in article_text:\n",
    "                found_identifiers.append(f\"company:{name}\")\n",
    "        \n",
    "        # Check search terms\n",
    "        for term in identifiers.get('search_terms', []):\n",
    "            if term.lower() in article_text:\n",
    "                found_identifiers.append(f\"search_term:{term}\")\n",
    "        \n",
    "        return found_identifiers\n",
    "    \n",
    "    def _calculate_company_name_score(self, article_text: str, company_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate relevance score based on company name matching\n",
    "        \"\"\"\n",
    "        if not company_name:\n",
    "            return 0.0\n",
    "        \n",
    "        company_name_lower = company_name.lower()\n",
    "        cleaned_name = self._clean_company_name(company_name).lower()\n",
    "        \n",
    "        score = 0.0\n",
    "        \n",
    "        # Exact company name match\n",
    "        if company_name_lower in article_text:\n",
    "            score += 1.0\n",
    "        \n",
    "        # Cleaned company name match\n",
    "        elif cleaned_name and cleaned_name in article_text:\n",
    "            score += 0.8\n",
    "        \n",
    "        # Partial matching (first two words)\n",
    "        else:\n",
    "            words = cleaned_name.split()\n",
    "            if len(words) >= 2:\n",
    "                first_two = ' '.join(words[:2])\n",
    "                if first_two in article_text:\n",
    "                    score += 0.5\n",
    "            \n",
    "            # Individual word matching\n",
    "            word_matches = 0\n",
    "            significant_words = [w for w in words if len(w) > 3 and w not in self.stop_words]\n",
    "            \n",
    "            for word in significant_words:\n",
    "                if word in article_text:\n",
    "                    word_matches += 1\n",
    "            \n",
    "            if significant_words:\n",
    "                score += (word_matches / len(significant_words)) * 0.3\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def _calculate_keyword_score(self, article_text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate relevance score based on financial keywords\n",
    "        \"\"\"\n",
    "        total_score = 0.0\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        for category, keywords in self.financial_keywords.items():\n",
    "            if category == 'high_weight':\n",
    "                weight = 3.0\n",
    "            elif category == 'medium_weight':\n",
    "                weight = 2.0\n",
    "            else:  # low_weight\n",
    "                weight = 1.0\n",
    "            \n",
    "            found_keywords = 0\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in article_text:\n",
    "                    found_keywords += 1\n",
    "            \n",
    "            if keywords:\n",
    "                category_score = (found_keywords / len(keywords)) * weight\n",
    "                total_score += category_score\n",
    "                total_weight += weight\n",
    "        \n",
    "        return (total_score / total_weight) if total_weight > 0 else 0.0\n",
    "    \n",
    "    def _calculate_context_score(self, title: str, description: str, company_name: str, ticker: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate contextual relevance (title gets higher weight)\n",
    "        \"\"\"\n",
    "        title_lower = title.lower()\n",
    "        description_lower = description.lower()\n",
    "        \n",
    "        score = 0.0\n",
    "        \n",
    "        # Company mentions in title (higher weight)\n",
    "        if ticker and ticker.lower() in title_lower:\n",
    "            score += 0.6\n",
    "        elif company_name and company_name.lower() in title_lower:\n",
    "            score += 0.4\n",
    "        \n",
    "        # Company mentions in description\n",
    "        if ticker and ticker.lower() in description_lower:\n",
    "            score += 0.3\n",
    "        elif company_name and company_name.lower() in description_lower:\n",
    "            score += 0.2\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def deduplicate_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Remove duplicate articles from multiple sources\n",
    "        Uses title similarity and URL matching\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not articles:\n",
    "                return []\n",
    "            \n",
    "            logger.debug(f\"Deduplicating {len(articles)} articles\")\n",
    "            \n",
    "            # Track seen articles\n",
    "            seen_hashes = set()\n",
    "            seen_urls = set()\n",
    "            seen_titles = {}\n",
    "            unique_articles = []\n",
    "            \n",
    "            for article in articles:\n",
    "                # Generate content hash\n",
    "                content_hash = self._generate_article_hash(article)\n",
    "                \n",
    "                # Check URL duplicates\n",
    "                url = self._clean_url(article.get('link', ''))\n",
    "                \n",
    "                # Check title similarity\n",
    "                title = self._clean_text(article.get('title', ''))\n",
    "                title_normalized = self._normalize_title(title)\n",
    "                \n",
    "                is_duplicate = False\n",
    "                \n",
    "                # Check against various duplicate criteria\n",
    "                if content_hash in seen_hashes:\n",
    "                    is_duplicate = True\n",
    "                    logger.debug(f\"Content hash duplicate: {title[:50]}...\")\n",
    "                \n",
    "                elif url and url in seen_urls:\n",
    "                    is_duplicate = True\n",
    "                    logger.debug(f\"URL duplicate: {title[:50]}...\")\n",
    "                \n",
    "                elif title_normalized:\n",
    "                    # Check for similar titles\n",
    "                    for existing_title_norm, existing_article in seen_titles.items():\n",
    "                        similarity = self._calculate_title_similarity(title_normalized, existing_title_norm)\n",
    "                        if similarity > 0.85:  # 85% similarity threshold\n",
    "                            is_duplicate = True\n",
    "                            logger.debug(f\"Title similarity duplicate: {title[:50]}...\")\n",
    "                            \n",
    "                            # Keep the one with higher relevance score\n",
    "                            if article.get('relevance_score', 0) > existing_article.get('relevance_score', 0):\n",
    "                                # Replace the existing article\n",
    "                                unique_articles.remove(existing_article)\n",
    "                                seen_titles[title_normalized] = article\n",
    "                                unique_articles.append(article)\n",
    "                            break\n",
    "                \n",
    "                if not is_duplicate:\n",
    "                    seen_hashes.add(content_hash)\n",
    "                    if url:\n",
    "                        seen_urls.add(url)\n",
    "                    if title_normalized:\n",
    "                        seen_titles[title_normalized] = article\n",
    "                    unique_articles.append(article)\n",
    "            \n",
    "            logger.info(f\"Deduplication: {len(articles)} -> {len(unique_articles)} articles\")\n",
    "            return unique_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in deduplicate_articles: {str(e)}\")\n",
    "            return articles  # Return original articles if deduplication fails\n",
    "    \n",
    "    def _generate_article_hash(self, article: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Generate hash for article content comparison\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use title and first part of description for hash\n",
    "            title = self._clean_text(article.get('title', ''))\n",
    "            description = self._clean_text(article.get('description', ''))[:200]  # First 200 chars\n",
    "            \n",
    "            content = f\"{title}|{description}\".lower()\n",
    "            return hashlib.md5(content.encode()).hexdigest()\n",
    "            \n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    \n",
    "    def _clean_url(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize URL for comparison\n",
    "        \"\"\"\n",
    "        if not url:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove common tracking parameters\n",
    "        url = re.sub(r'[?&](utm_|ref=|source=|campaign=)[^&]*', '', url)\n",
    "        \n",
    "        # Remove trailing slashes and fragments\n",
    "        url = re.sub(r'[/#]+$', '', url)\n",
    "        \n",
    "        return url.lower().strip()\n",
    "    \n",
    "    def _normalize_title(self, title: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize title for similarity comparison\n",
    "        \"\"\"\n",
    "        if not title:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase and remove extra spaces\n",
    "        normalized = re.sub(r'\\s+', ' ', title.lower().strip())\n",
    "        \n",
    "        # Remove common punctuation\n",
    "        normalized = re.sub(r'[^\\w\\s]', ' ', normalized)\n",
    "        \n",
    "        # Remove stop words\n",
    "        words = normalized.split()\n",
    "        significant_words = [w for w in words if len(w) > 2 and w not in self.stop_words]\n",
    "        \n",
    "        return ' '.join(significant_words)\n",
    "    \n",
    "    def _calculate_title_similarity(self, title1: str, title2: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate similarity between two normalized titles\n",
    "        \"\"\"\n",
    "        if not title1 or not title2:\n",
    "            return 0.0\n",
    "        \n",
    "        words1 = set(title1.split())\n",
    "        words2 = set(title2.split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def sort_by_recency(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Sort articles by relevance score and recency (newest first for FinBERT)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not articles:\n",
    "                return []\n",
    "            \n",
    "            logger.debug(f\"Sorting {len(articles)} articles by relevance and recency\")\n",
    "            \n",
    "            # Parse dates and add sort keys\n",
    "            for article in articles:\n",
    "                parsed_date = self._parse_article_date(article.get('published', ''))\n",
    "                article['_parsed_date'] = parsed_date\n",
    "                article['_relevance_score'] = article.get('relevance_score', 0.0)\n",
    "            \n",
    "            # Sort by relevance (desc) then by recency (desc)\n",
    "            sorted_articles = sorted(\n",
    "                articles,\n",
    "                key=lambda x: (x['_relevance_score'], x['_parsed_date']),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Remove temporary sort keys\n",
    "            for article in sorted_articles:\n",
    "                if '_parsed_date' in article:\n",
    "                    del article['_parsed_date']\n",
    "                if '_relevance_score' in article:\n",
    "                    del article['_relevance_score']\n",
    "            \n",
    "            return sorted_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error sorting articles: {str(e)}\")\n",
    "            return articles\n",
    "    \n",
    "    def _parse_article_date(self, date_str: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Parse article publication date string to datetime object\n",
    "        \"\"\"\n",
    "        if not date_str:\n",
    "            return datetime.min.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        try:\n",
    "            # Try ISO format first\n",
    "            if 'T' in date_str and 'Z' in date_str:\n",
    "                return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "            \n",
    "            # Try ISO format without Z\n",
    "            if 'T' in date_str:\n",
    "                return datetime.fromisoformat(date_str)\n",
    "            \n",
    "            # Try common date formats\n",
    "            common_formats = [\n",
    "                '%Y-%m-%d %H:%M:%S',\n",
    "                '%Y-%m-%d',\n",
    "                '%d/%m/%Y',\n",
    "                '%m/%d/%Y',\n",
    "                '%Y-%m-%dT%H:%M:%S%z'\n",
    "            ]\n",
    "            \n",
    "            for fmt in common_formats:\n",
    "                try:\n",
    "                    return datetime.strptime(date_str, fmt).replace(tzinfo=timezone.utc)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # Default to current time if parsing fails\n",
    "            logger.debug(f\"Could not parse date: {date_str}\")\n",
    "            return datetime.now(timezone.utc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Date parsing error: {str(e)}\")\n",
    "            return datetime.now(timezone.utc)\n",
    "    \n",
    "    def _filter_by_date(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Filter articles by date (keep only recent articles)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not articles:\n",
    "                return []\n",
    "            \n",
    "            cutoff_date = datetime.now(timezone.utc) - timedelta(days=self.max_days_old)\n",
    "            \n",
    "            filtered_articles = []\n",
    "            for article in articles:\n",
    "                article_date = self._parse_article_date(article.get('published', ''))\n",
    "                \n",
    "                if article_date >= cutoff_date:\n",
    "                    filtered_articles.append(article)\n",
    "                else:\n",
    "                    logger.debug(f\"Filtered old article: {article.get('title', '')[:50]}...\")\n",
    "            \n",
    "            logger.info(f\"Date filtering: {len(articles)} -> {len(filtered_articles)} articles (max {self.max_days_old} days old)\")\n",
    "            return filtered_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error filtering by date: {str(e)}\")\n",
    "            return articles\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text content\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Normalize unicode\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        # Remove HTML tags if any\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_filter_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get filter configuration and statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'configuration': {\n",
    "                'relevance_threshold': self.relevance_threshold,\n",
    "                'max_articles_per_company': self.max_articles_per_company,\n",
    "                'max_days_old': self.max_days_old\n",
    "            },\n",
    "            'keyword_categories': {\n",
    "                category: len(keywords) \n",
    "                for category, keywords in self.financial_keywords.items()\n",
    "            },\n",
    "            'total_keywords': sum(len(keywords) for keywords in self.financial_keywords.values()),\n",
    "            'stop_words_count': len(self.stop_words)\n",
    "        }\n",
    "\n",
    "# Convenience function for quick usage\n",
    "def create_content_filter(relevance_threshold=0.6, max_articles=50, max_days=7) -> ContentFilter:\n",
    "    \"\"\"Factory function to create ContentFilter instance\"\"\"\n",
    "    return ContentFilter(relevance_threshold, max_articles, max_days)\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize content filter\n",
    "        content_filter = ContentFilter()\n",
    "        \n",
    "        print(\"=== Content Filter Test ===\")\n",
    "        \n",
    "        # Test configuration\n",
    "        print(\"\\n1. Filter Configuration:\")\n",
    "        stats = content_filter.get_filter_stats()\n",
    "        print(f\"   Relevance Threshold: {stats['configuration']['relevance_threshold']}\")\n",
    "        print(f\"   Max Articles: {stats['configuration']['max_articles_per_company']}\")\n",
    "        print(f\"   Max Days Old: {stats['configuration']['max_days_old']}\")\n",
    "        print(f\"   Total Keywords: {stats['total_keywords']}\")\n",
    "        \n",
    "        # Sample articles for testing\n",
    "        sample_articles = [\n",
    "            {\n",
    "                'title': 'TCS Q3 Results: Revenue grows 12% YoY, profit jumps 15%',\n",
    "                'description': 'Tata Consultancy Services reported strong quarterly results with revenue growth of 12% year-on-year and profit increase of 15%. The company maintained its market leadership position.',\n",
    "                'link': 'https://example.com/tcs-results-1',\n",
    "                'published': '2025-01-15T10:30:00Z',\n",
    "                'source': 'Economic Times'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Indian IT sector shows resilience amid global challenges',\n",
    "                'description': 'The Indian IT sector, led by companies like TCS, Infosys, and Wipro, continues to show strong performance despite global economic uncertainties.',\n",
    "                'link': 'https://example.com/it-sector-news',\n",
    "                'published': '2025-01-14T14:20:00Z',\n",
    "                'source': 'Business Standard'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Weather update: Heavy rainfall expected in Mumbai',\n",
    "                'description': 'The meteorological department has issued a warning for heavy rainfall in Mumbai and surrounding areas for the next 48 hours.',\n",
    "                'link': 'https://example.com/weather-news',\n",
    "                'published': '2025-01-15T08:00:00Z',\n",
    "                'source': 'Moneycontrol'\n",
    "            },\n",
    "            {\n",
    "                'title': 'TCS announces major expansion plans, to hire 50000 employees',\n",
    "                'description': 'Tata Consultancy Services has announced significant expansion plans with a focus on emerging technologies. The company plans to hire 50,000 new employees over the next year.',\n",
    "                'link': 'https://example.com/tcs-expansion',\n",
    "                'published': '2025-01-13T16:45:00Z',\n",
    "                'source': 'Economic Times'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Sample company data\n",
    "        company_data = {\n",
    "            'symbol': 'TCS',\n",
    "            'nse_symbol': 'TCS',\n",
    "            'company_name': 'Tata Consultancy Services Limited',\n",
    "            'search_terms': ['TCS', 'Tata Consultancy Services', 'Tata Consultancy']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n2. Testing with {len(sample_articles)} sample articles for {company_data['company_name']}\")\n",
    "        \n",
    "        # Test individual article relevance\n",
    "        print(\"\\n3. Individual Article Relevance Scores:\")\n",
    "        for i, article in enumerate(sample_articles, 1):\n",
    "            score = content_filter.check_article_relevance(\n",
    "                article, \n",
    "                company_data['company_name'], \n",
    "                company_data['symbol']\n",
    "            )\n",
    "            print(f\"   {i}. Score: {score:.3f} - {article['title'][:60]}...\")\n",
    "        \n",
    "        # Test complete filtering pipeline\n",
    "        print(\"\\n4. Complete Filtering Pipeline:\")\n",
    "        filtered_articles = content_filter.filter_company_articles(sample_articles, company_data)\n",
    "        \n",
    "        print(f\"   Input Articles: {len(sample_articles)}\")\n",
    "        print(f\"   Filtered Articles: {len(filtered_articles)}\")\n",
    "        \n",
    "        if filtered_articles:\n",
    "            print(\"\\n   Filtered Results:\")\n",
    "            for i, article in enumerate(filtered_articles, 1):\n",
    "                print(f\"   {i}. {article['title']}\")\n",
    "                print(f\"      Relevance: {article.get('relevance_score', 0):.3f}\")\n",
    "                print(f\"      Source: {article.get('source', 'Unknown')}\")\n",
    "                print(f\"      Identifiers: {', '.join(article.get('company_identifiers_found', []))}\")\n",
    "        \n",
    "        # Test deduplication\n",
    "        print(\"\\n5. Deduplication Test:\")\n",
    "        # Add duplicate article\n",
    "        duplicate_article = sample_articles[0].copy()\n",
    "        duplicate_article['link'] = 'https://example.com/tcs-results-duplicate'\n",
    "        duplicate_article['source'] = 'Different Source'\n",
    "        \n",
    "        test_articles_with_duplicate = sample_articles + [duplicate_article]\n",
    "        deduplicated = content_filter.deduplicate_articles(test_articles_with_duplicate)\n",
    "        \n",
    "        print(f\"   Before deduplication: {len(test_articles_with_duplicate)} articles\")\n",
    "        print(f\"   After deduplication: {len(deduplicated)} articles\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
