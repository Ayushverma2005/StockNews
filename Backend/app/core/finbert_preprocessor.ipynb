{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finbert_preprocessor.py - FinBERT Ready Output (Fixed)\n",
    "import re\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FinBERTPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessor for preparing filtered news articles for FinBERT sentiment analysis.\n",
    "    Handles context-aware chunking and API-ready batch formatting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens_per_chunk: int = 400, chunk_overlap: int = 50, batch_size: int = 10):\n",
    "        self.max_tokens_per_chunk = max_tokens_per_chunk\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def prepare_for_finbert(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Main entry point for preparing articles for FinBERT analysis.\n",
    "        \n",
    "        Args:\n",
    "            articles: List of filtered, deduplicated, and sorted articles from content_filter.py\n",
    "        \n",
    "        Returns:\n",
    "            Flat list of preprocessed articles ready for FinBERT client\n",
    "        \"\"\"\n",
    "        if not articles:\n",
    "            logger.warning(\"No articles provided for FinBERT preprocessing\")\n",
    "            return []\n",
    "        \n",
    "        logger.info(f\"Starting FinBERT preprocessing for {len(articles)} articles\")\n",
    "        \n",
    "        try:\n",
    "            # Select top articles\n",
    "            top_articles = self._select_top_articles(articles)\n",
    "            logger.info(f\"Selected {len(top_articles)} articles for processing\")\n",
    "            \n",
    "            # Clean and prepare article text\n",
    "            cleaned_articles = self._clean_articles(top_articles)\n",
    "            logger.info(f\"Cleaning complete: {len(cleaned_articles)} articles retained, {len(top_articles) - len(cleaned_articles)} skipped for empty text\")\n",
    "            \n",
    "            # Chunk long articles with context awareness\n",
    "            chunked_articles = self.chunk_long_articles(cleaned_articles)\n",
    "            logger.info(f\"Created {len(chunked_articles)} chunks from {len(cleaned_articles)} cleaned articles\")\n",
    "            \n",
    "            # Convert to flat list format for FinBERT client\n",
    "            finbert_ready = self._convert_to_finbert_format(chunked_articles)\n",
    "            \n",
    "            logger.info(f\"Prepared {len(finbert_ready)} text entries for FinBERT\")\n",
    "            return finbert_ready\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in prepare_for_finbert: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _select_top_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Select top 15 articles prioritizing relevance and recency.\"\"\"\n",
    "        try:\n",
    "            # Articles are already sorted by recency from content_filter\n",
    "            # Take top 15 for optimal API usage and processing time\n",
    "            return articles[:15]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error selecting top articles: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _clean_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Clean article text for better FinBERT processing.\"\"\"\n",
    "        cleaned = []\n",
    "        \n",
    "        for article in articles:\n",
    "            try:\n",
    "                cleaned_article = article.copy()\n",
    "                \n",
    "                # Clean title - handle None/empty values\n",
    "                title = article.get('title', '') or ''\n",
    "                cleaned_article['title'] = self._clean_text(str(title))\n",
    "                \n",
    "                # Clean content - handle None/empty values\n",
    "                content = article.get('description', '') or ''\n",
    "                cleaned_article['content'] = self._clean_text(str(content))\n",
    "\n",
    "                # Normalize/propagate metadata expected downstream\n",
    "                # Map 'published' -> 'published_date' for consistent usage\n",
    "                if 'published_date' not in cleaned_article:\n",
    "                    cleaned_article['published_date'] = article.get('published', '') or article.get('published_date', '')\n",
    "                # Ensure source present\n",
    "                cleaned_article['source'] = article.get('source', cleaned_article.get('source', 'unknown')) or 'unknown'\n",
    "                # Keep link if available for unique identification\n",
    "                cleaned_article['link'] = article.get('link', cleaned_article.get('link', ''))\n",
    "                \n",
    "                # Combine title and content for processing\n",
    "                combined_text = \"\"\n",
    "                if cleaned_article['title']:\n",
    "                    combined_text += cleaned_article['title'] + \". \"\n",
    "                if cleaned_article['content']:\n",
    "                    combined_text += cleaned_article['content']\n",
    "                \n",
    "                cleaned_article['full_text'] = combined_text.strip()\n",
    "                \n",
    "                # Only add if there's actual content\n",
    "                if cleaned_article['full_text']:\n",
    "                    cleaned.append(cleaned_article)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping article with no content: {article.get('title', 'Unknown')}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error cleaning article: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text for FinBERT processing.\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # Remove HTML tags\n",
    "            text = re.sub(r'<[^>]+>', '', text)\n",
    "            \n",
    "            # Clean up whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = re.sub(r'\\n+', ' ', text)\n",
    "            \n",
    "            # Normalize financial symbols and currency\n",
    "            text = re.sub(r'₹\\s*(\\d+)', r'INR \\1', text)  # Indian Rupee\n",
    "            text = re.sub(r'\\$\\s*(\\d+)', r'USD \\1', text)  # US Dollar\n",
    "            \n",
    "            # Clean up quotes and special characters\n",
    "            text = re.sub(r'[\"\"\"]', '\"', text)\n",
    "            text = re.sub(r\"['']\", \"'\", text)\n",
    "            \n",
    "            return text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cleaning text: {str(e)}\")\n",
    "            return text  # Return original text if cleaning fails\n",
    "    \n",
    "    def chunk_long_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Context-aware chunking of articles for FinBERT token limits.\n",
    "        Preserves sentence boundaries and maintains financial context.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for article in articles:\n",
    "            try:\n",
    "                full_text = article.get('full_text', '')\n",
    "                if not full_text:\n",
    "                    logger.debug(\"Skipping chunking for article with empty full_text\")\n",
    "                    continue\n",
    "                \n",
    "                # Estimate tokens (rough approximation: 1 token ≈ 4 characters)\n",
    "                estimated_tokens = len(full_text) / 4\n",
    "                \n",
    "                if estimated_tokens <= self.max_tokens_per_chunk:\n",
    "                    # Article is short enough, use as single chunk\n",
    "                    chunk = {\n",
    "                        'text': full_text,\n",
    "                        'source': str(article.get('source', 'unknown')),\n",
    "                        'published_date': article.get('published_date'),\n",
    "                        'original_title': str(article.get('title', '')),\n",
    "                        'link': article.get('link', ''),\n",
    "                        'chunk_id': 0,\n",
    "                        'total_chunks': 1\n",
    "                    }\n",
    "                    chunks.append(chunk)\n",
    "                else:\n",
    "                    # Article needs chunking\n",
    "                    article_chunks = self._create_context_aware_chunks(full_text, article)\n",
    "                    chunks.extend(article_chunks)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error chunking article: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_context_aware_chunks(self, text: str, article: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create overlapping chunks while preserving sentence boundaries.\"\"\"\n",
    "        try:\n",
    "            sentences = self._split_into_sentences(text)\n",
    "            if not sentences:\n",
    "                return []\n",
    "            \n",
    "            chunks = []\n",
    "            chunk_id = 0\n",
    "            \n",
    "            current_chunk = \"\"\n",
    "            current_chunk_tokens = 0\n",
    "            \n",
    "            i = 0\n",
    "            while i < len(sentences):\n",
    "                sentence = sentences[i]\n",
    "                sentence_tokens = len(sentence) / 4  # Rough token estimate\n",
    "                \n",
    "                # Check if adding this sentence would exceed token limit\n",
    "                if current_chunk_tokens + sentence_tokens > self.max_tokens_per_chunk and current_chunk:\n",
    "                    # Create chunk from current content\n",
    "                    chunk = {\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'source': str(article.get('source', 'unknown')),\n",
    "                        'published_date': article.get('published_date'),\n",
    "                        'original_title': str(article.get('title', '')),\n",
    "                        'link': article.get('link', ''),\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'total_chunks': 0  # Will be updated after all chunks are created\n",
    "                    }\n",
    "                    chunks.append(chunk)\n",
    "                    chunk_id += 1\n",
    "                    \n",
    "                    # Start new chunk with overlap\n",
    "                    overlap_text = self._get_overlap_text(current_chunk)\n",
    "                    current_chunk = overlap_text + \" \" + sentence if overlap_text else sentence\n",
    "                    current_chunk_tokens = len(current_chunk) / 4\n",
    "                else:\n",
    "                    # Add sentence to current chunk\n",
    "                    current_chunk += (\" \" + sentence) if current_chunk else sentence\n",
    "                    current_chunk_tokens += sentence_tokens\n",
    "                \n",
    "                i += 1\n",
    "            \n",
    "            # Add final chunk if it has content\n",
    "            if current_chunk.strip():\n",
    "                chunk = {\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'source': str(article.get('source', 'unknown')),\n",
    "                    'published_date': article.get('published_date'),\n",
    "                    'original_title': str(article.get('title', '')),\n",
    "                    'link': article.get('link', ''),\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'total_chunks': 0\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            # Update total_chunks count for all chunks of this article\n",
    "            total_chunks = len(chunks)\n",
    "            article_title = str(article.get('title', ''))\n",
    "            for chunk in chunks:\n",
    "                if chunk['original_title'] == article_title:\n",
    "                    chunk['total_chunks'] = total_chunks\n",
    "            \n",
    "            return chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating context-aware chunks: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences, handling financial context.\"\"\"\n",
    "        try:\n",
    "            if not text:\n",
    "                return []\n",
    "            \n",
    "            # Basic sentence splitting that preserves financial abbreviations\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "            \n",
    "            # Clean up sentences\n",
    "            cleaned_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if sentence:  # Check if sentence exists\n",
    "                    sentence = sentence.strip()\n",
    "                    if sentence and len(sentence) > 10:  # Filter out very short fragments\n",
    "                        cleaned_sentences.append(sentence)\n",
    "            \n",
    "            return cleaned_sentences\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error splitting sentences: {str(e)}\")\n",
    "            return [text]  # Return original text as single sentence if splitting fails\n",
    "    \n",
    "    def _get_overlap_text(self, chunk_text: str) -> str:\n",
    "        \"\"\"Get overlap text from the end of current chunk.\"\"\"\n",
    "        try:\n",
    "            if not chunk_text:\n",
    "                return \"\"\n",
    "            \n",
    "            words = chunk_text.split()\n",
    "            overlap_words = max(1, int(self.chunk_overlap / 4))  # Ensure at least 1 word\n",
    "            \n",
    "            if len(words) > overlap_words:\n",
    "                return \" \".join(words[-overlap_words:])\n",
    "            return \"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting overlap text: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _convert_to_finbert_format(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Convert chunked articles to flat list format expected by FinBERT client.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunked articles\n",
    "        \n",
    "        Returns:\n",
    "            Flat list with format: [{'id': '...', 'text': '...', 'metadata': {...}}, ...]\n",
    "        \"\"\"\n",
    "        finbert_ready = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                # Safely construct chunk_id\n",
    "                source = str(chunk.get('source', 'unknown'))\n",
    "                title = str(chunk.get('original_title', 'untitled'))\n",
    "                chunk_id = chunk.get('chunk_id', 0)\n",
    "                \n",
    "                # Clean strings for use in ID\n",
    "                source_clean = re.sub(r'[^\\w\\-_]', '_', source)\n",
    "                title_clean = re.sub(r'[^\\w\\-_]', '_', title)[:50]\n",
    "                \n",
    "                item = {\n",
    "                    'id': f\"{source_clean}_{title_clean}_{chunk_id}\",\n",
    "                    'text': str(chunk.get('text', '')),\n",
    "                    'metadata': {\n",
    "                        'source': source,\n",
    "                        'published_date': chunk.get('published_date'),\n",
    "                        'title': title,\n",
    "                        'chunk_position': f\"{chunk_id + 1}/{chunk.get('total_chunks', 1)}\",\n",
    "                        'url': chunk.get('link', '')\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                finbert_ready.append(item)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting chunk to FinBERT format: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return finbert_ready\n",
    "\n",
    "# Usage example and integration point\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Example usage with mock data\n",
    "        preprocessor = FinBERTPreprocessor()\n",
    "        \n",
    "        # Mock articles from content_filter.py output\n",
    "        sample_articles = [\n",
    "            {\n",
    "                'title': 'Reliance Industries Reports Strong Q3 Results',\n",
    "                'description': 'Reliance Industries Limited announced robust quarterly results with revenue growth of 15% year-over-year. The company reported consolidated revenue of INR 2.3 lakh crore for the quarter ending December 2024.',\n",
    "                'source': 'Economic Times',\n",
    "                'published': '2024-01-15',\n",
    "                'relevance_score': 0.95\n",
    "            },\n",
    "            {\n",
    "                'title': None,  # Test None handling\n",
    "                'description': '',  # Test empty content\n",
    "                'source': 'Test Source',\n",
    "                'published': '2024-01-16',\n",
    "                'relevance_score': 0.8\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process articles\n",
    "        result = preprocessor.prepare_for_finbert(sample_articles)\n",
    "        \n",
    "        print(f\"\\nProcessing complete:\")\n",
    "        print(f\"- Total entries prepared: {len(result)}\")\n",
    "        \n",
    "        # Display first entry as example\n",
    "        if result:\n",
    "            print(f\"\\nExample entry:\")\n",
    "            print(f\"- ID: {result[0]['id']}\")\n",
    "            print(f\"- Text length: {len(result[0]['text'])} chars\")\n",
    "            print(f\"- Metadata: {result[0]['metadata']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        logger.error(f\"Main execution error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
