{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.py - Sentiment Analysis Pipeline Orchestrator\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Any\n",
    "from .user_input_processor import UserInputProcessor\n",
    "from .news_collector import NewsCollector\n",
    "from .finbert_preprocessor import FinBERTPreprocessor\n",
    "from .finbert_client import FinBERTClient\n",
    "from app.utils import resolve_path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SentimentPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end sentiment analysis pipeline orchestrator.\n",
    "    Workflow: User Input → News Collection → Preprocessing → Local FinBERT → Aggregation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize pipeline components.\n",
    "        \n",
    "        Args:\n",
    "            db_path: Path to NSE stocks database\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing Sentiment Pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            if db_path is None:\n",
    "                db_path = resolve_path(\"data/nse_stocks.db\")\n",
    "            \n",
    "            # Use separate paths for stock DB and cache DB\n",
    "            stocks_db_path = db_path\n",
    "            news_cache_db_path = resolve_path(\"data/news_cache.db\")   \n",
    "            \n",
    "            # Initialize components\n",
    "            self.user_input = UserInputProcessor(db_path=stocks_db_path)\n",
    "            self.news_collector = NewsCollector(database_path=news_cache_db_path)  \n",
    "            self.preprocessor = FinBERTPreprocessor()\n",
    "\n",
    "            # ✅ Minimal change: DON'T load FinBERT here\n",
    "            self.client = None\n",
    "            \n",
    "            logger.info(\"Pipeline initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing pipeline: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run(self, user_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute end-to-end sentiment analysis pipeline.\n",
    "        \n",
    "        Args:\n",
    "            user_query: Company name or symbol from user\n",
    "        \n",
    "        Returns:\n",
    "            Structured JSON result with company info, sentiment summary, and articles\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting pipeline for query: {user_query}\")\n",
    "            \n",
    "            # Step 1: Process user input and get company data\n",
    "            company_data = self.user_input.handle_user_selection(user_query)\n",
    "            if not company_data or company_data.get(\"error\"):\n",
    "                logger.warning(f\"User input failed: {company_data.get('error', 'Unknown error')}\")\n",
    "                return {\n",
    "                    \"stage\": \"user_input\",\n",
    "                    \"error\": company_data.get(\"error\", \"Company not found\")\n",
    "                }\n",
    "            \n",
    "            logger.info(f\"Company identified: {company_data.get('symbol')} - {company_data.get('name')}\")\n",
    "            \n",
    "            # Step 2: Collect news articles (RSS + Gemini + Cache)\n",
    "            news_result = self.news_collector.collect_company_news(company_data)\n",
    "            articles = news_result.get(\"articles\", [])\n",
    "            \n",
    "            if not articles:\n",
    "                logger.warning(\"No articles found for company\")\n",
    "                return {\n",
    "                    \"stage\": \"news_collection\",\n",
    "                    \"error\": \"No articles found\",\n",
    "                    \"company\": company_data\n",
    "                }\n",
    "            \n",
    "            logger.info(f\"Collected {len(articles)} articles\")\n",
    "            \n",
    "            # Step 3: Preprocess articles for FinBERT\n",
    "            preprocessed_chunks = self.preprocessor.prepare_for_finbert(articles)\n",
    "            \n",
    "            if not preprocessed_chunks:\n",
    "                logger.warning(\"No valid articles after preprocessing\")\n",
    "                return {\n",
    "                    \"stage\": \"preprocessing\",\n",
    "                    \"error\": \"No valid articles after preprocessing\",\n",
    "                    \"company\": company_data\n",
    "                }\n",
    "            \n",
    "            logger.info(f\"Preprocessed {len(preprocessed_chunks)} chunks\")\n",
    "            \n",
    "            # Step 4: Run local FinBERT sentiment analysis\n",
    "            # ✅ Minimal change: Lazy-load FinBERT client only when needed\n",
    "            if self.client is None:\n",
    "                self.client = FinBERTClient()\n",
    "\n",
    "            predictions = self.client.analyze(preprocessed_chunks)\n",
    "            \n",
    "            if not predictions:\n",
    "                logger.warning(\"No predictions returned from FinBERT\")\n",
    "                return {\n",
    "                    \"stage\": \"sentiment_inference\",\n",
    "                    \"error\": \"No predictions returned\",\n",
    "                    \"company\": company_data\n",
    "                }\n",
    "            \n",
    "            logger.info(f\"Generated {len(predictions)} sentiment predictions\")\n",
    "            \n",
    "            # Step 5: Aggregate results into final structured output\n",
    "            final_result = self._aggregate_results(company_data, preprocessed_chunks, predictions)\n",
    "            \n",
    "            logger.info(f\"Pipeline complete: {final_result['sentiment_summary']['sentiment_label']}\")\n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(\"Pipeline execution failed\")\n",
    "            return {\n",
    "                \"stage\": \"pipeline\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _aggregate_results(self, \n",
    "                          company_data: Dict[str, Any],\n",
    "                          preprocessed_articles: List[Dict[str, Any]],\n",
    "                          predictions: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Aggregate sentiment predictions with article metadata into final JSON output.\n",
    "        \n",
    "        Args:\n",
    "            company_data: Company information\n",
    "            preprocessed_articles: List of preprocessed articles with metadata\n",
    "            predictions: List of sentiment predictions from FinBERT\n",
    "        \n",
    "        Returns:\n",
    "            Structured JSON response for API/frontend consumption\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Build prediction lookup by ID\n",
    "            prediction_map = {pred['id']: pred for pred in predictions}\n",
    "            \n",
    "            # Merge predictions with article metadata\n",
    "            enriched_articles = []\n",
    "            sentiment_counts = {\"bullish\": 0, \"neutral\": 0, \"bearish\": 0}\n",
    "            total_positive = 0.0\n",
    "            total_negative = 0.0\n",
    "            \n",
    "            for article in preprocessed_articles:\n",
    "                article_id = article['id']\n",
    "                prediction = prediction_map.get(article_id)\n",
    "                \n",
    "                if not prediction:\n",
    "                    logger.warning(f\"No prediction found for article ID: {article_id}\")\n",
    "                    continue\n",
    "                \n",
    "                metadata = article.get('metadata', {})\n",
    "                sentiment = prediction['sentiment']\n",
    "                scores = prediction['scores']\n",
    "                \n",
    "                # Map sentiment to bullish/bearish/neutral\n",
    "                if sentiment == 'positive':\n",
    "                    sentiment_label = 'bullish'\n",
    "                elif sentiment == 'negative':\n",
    "                    sentiment_label = 'bearish'\n",
    "                else:\n",
    "                    sentiment_label = 'neutral'\n",
    "                \n",
    "                sentiment_counts[sentiment_label] += 1\n",
    "                total_positive += scores['positive']\n",
    "                total_negative += scores['negative']\n",
    "                \n",
    "                # Build enriched article dict\n",
    "                enriched_article = {\n",
    "                    \"title\": metadata.get('title', 'Unknown'),\n",
    "                    \"url\": metadata.get('url', ''),\n",
    "                    \"source\": metadata.get('source', 'unknown'),\n",
    "                    \"published\": metadata.get('published_date', ''),\n",
    "                    \"sentiment\": sentiment_label,\n",
    "                    \"positive\": round(scores['positive'], 4),\n",
    "                    \"neutral\": round(scores['neutral'], 4),\n",
    "                    \"negative\": round(scores['negative'], 4)\n",
    "                }\n",
    "                \n",
    "                enriched_articles.append(enriched_article)\n",
    "            \n",
    "            # Calculate overall sentiment metrics\n",
    "            article_count = len(enriched_articles)\n",
    "            \n",
    "            if article_count > 0:\n",
    "                # Overall sentiment score: (avg positive - avg negative)\n",
    "                avg_positive = total_positive / article_count\n",
    "                avg_negative = total_negative / article_count\n",
    "                overall_score = avg_positive - avg_negative\n",
    "                \n",
    "                # Classify overall sentiment\n",
    "                if overall_score >= 0.2:\n",
    "                    overall_label = \"bullish\"\n",
    "                elif overall_score <= -0.2:\n",
    "                    overall_label = \"bearish\"\n",
    "                else:\n",
    "                    overall_label = \"neutral\"\n",
    "                \n",
    "                # Calculate confidence (based on score magnitude and agreement)\n",
    "                confidence = min(abs(overall_score) + 0.3, 1.0)  # Simple heuristic\n",
    "                \n",
    "            else:\n",
    "                overall_score = 0.0\n",
    "                overall_label = \"neutral\"\n",
    "                confidence = 0.0\n",
    "            \n",
    "            # Build final structured response\n",
    "            final_result = {\n",
    "                \"company\": {\n",
    "                    \"symbol\": company_data.get('symbol', 'UNKNOWN'),\n",
    "                    \"name\": company_data.get('company_name', 'Unknown Company'),\n",
    "                    \"sector\": company_data.get('sector', 'Unknown')\n",
    "                },\n",
    "                \"article_count\": article_count,\n",
    "                \"sentiment_summary\": {\n",
    "                    \"bullish\": sentiment_counts[\"bullish\"],\n",
    "                    \"neutral\": sentiment_counts[\"neutral\"],\n",
    "                    \"bearish\": sentiment_counts[\"bearish\"],\n",
    "                    \"overall_score\": round(overall_score, 4),\n",
    "                    \"sentiment_label\": overall_label,\n",
    "                    \"confidence\": round(confidence, 4)\n",
    "                },\n",
    "                \"articles\": enriched_articles\n",
    "            }\n",
    "            \n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error aggregating results: {str(e)}\")\n",
    "            return {\n",
    "                \"company\": company_data,\n",
    "                \"article_count\": 0,\n",
    "                \"sentiment_summary\": {\n",
    "                    \"bullish\": 0,\n",
    "                    \"neutral\": 0,\n",
    "                    \"bearish\": 0,\n",
    "                    \"overall_score\": 0.0,\n",
    "                    \"sentiment_label\": \"neutral\",\n",
    "                    \"confidence\": 0.0\n",
    "                },\n",
    "                \"articles\": [],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "\n",
    "class PipelineCLI:\n",
    "    \"\"\"CLI runner for the SentimentPipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize CLI (no API token needed for local FinBERT)\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def parse_args(self):\n",
    "        \"\"\"Parse command line arguments\"\"\"\n",
    "        parser = argparse.ArgumentParser(\n",
    "            description=\"Run sentiment analysis pipeline for NSE companies\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"symbols\",\n",
    "            nargs=\"+\",\n",
    "            help=\"Company symbols or names (e.g., RELIANCE, TCS, 'Tata Motors')\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--verbose\",\n",
    "            action=\"store_true\",\n",
    "            help=\"Enable debug logging\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--db-path\",\n",
    "            default=\"data/nse_stocks.db\",\n",
    "            help=\"Path to NSE stocks database\"\n",
    "        )\n",
    "        return parser.parse_args()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute CLI workflow\"\"\"\n",
    "        args = self.parse_args()\n",
    "        \n",
    "        # Set logging level\n",
    "        if args.verbose:\n",
    "            logging.getLogger().setLevel(logging.DEBUG)\n",
    "        \n",
    "        # Initialize pipeline\n",
    "        try:\n",
    "            pipeline = SentimentPipeline(db_path=args.db_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize pipeline: {str(e)}\")\n",
    "            return\n",
    "        \n",
    "        # Process each symbol\n",
    "        results = {}\n",
    "        for symbol in args.symbols:\n",
    "            logger.info(f\"\\n{'='*60}\")\n",
    "            logger.info(f\"Processing: {symbol}\")\n",
    "            logger.info(f\"{'='*60}\\n\")\n",
    "            \n",
    "            result = pipeline.run(symbol)\n",
    "            results[symbol] = result\n",
    "        \n",
    "        # Print final results as JSON\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTS\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        print(json.dumps(results, indent=2))\n",
    "\n",
    "\n",
    "# Test/Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run CLI if called directly\n",
    "    PipelineCLI().run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
