{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e05b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "News Collector Module for NSE Stock Market Sentiment Analysis\n",
    "Handles news collection, caching, and preparation for FinBERT processing\n",
    "\n",
    "Dependencies: rss_manager.py, content_filter.py, news_database.py\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any\n",
    "import sqlite3\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from .llm_web_searcher import LLMWebSearcher  \n",
    "from app.utils import resolve_path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NewsCollector:\n",
    "    \"\"\"\n",
    "    Main news collection orchestrator that coordinates between\n",
    "    user input processing and downstream sentiment analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, database_path: str = None, cache_expiry_days: int = 3):\n",
    "        # Resolve to an absolute path anchored at project root\n",
    "        # If no path is provided, use default path resolved from project root\n",
    "        if database_path is None:\n",
    "            self.db_path = resolve_path(\"data/news_cache.db\")\n",
    "        elif os.path.isabs(database_path):\n",
    "            self.db_path = database_path\n",
    "        else:\n",
    "            self.db_path = resolve_path(database_path)\n",
    "            \n",
    "        self.cache_expiry_days = cache_expiry_days\n",
    "        self.max_articles_per_company = 15\n",
    "        self.min_articles_required = 1\n",
    "        \n",
    "        # Initialize supporting modules (will be imported when available)\n",
    "        self.rss_manager = None\n",
    "        self.content_filter = None\n",
    "        self.news_database = None\n",
    "        \n",
    "        self._initialize_modules()\n",
    "        self.llm_searcher = LLMWebSearcher()\n",
    "        self._ensure_news_tables()\n",
    "    \n",
    "    def _initialize_modules(self):\n",
    "        \"\"\"Initialize supporting modules\"\"\"\n",
    "        try:\n",
    "            # Import RSS Manager\n",
    "            from .rss_manager import RSSManager\n",
    "            self.rss_manager = RSSManager()\n",
    "            \n",
    "            #from content_filter import ContentFilter\n",
    "            #self.content_filter = ContentFilter()\n",
    "            #from news_database import NewsDatabase\n",
    "            #self.news_database = NewsDatabase(self.db_path)\n",
    "            \n",
    "            logger.info(\"Supporting modules initialized successfully\")\n",
    "        except ImportError as e:\n",
    "            logger.warning(f\"Some modules not yet available: {e}\")\n",
    "            # For MVP, we'll implement basic functionality inline\n",
    "    \n",
    "    def _ensure_news_tables(self):\n",
    "        \"\"\"Create news cache tables if they don't exist\"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                # News cache table\n",
    "                cursor.execute('''\n",
    "                    CREATE TABLE IF NOT EXISTS news_cache (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        company_symbol TEXT NOT NULL,\n",
    "                        article_title TEXT NOT NULL,\n",
    "                        article_content TEXT,\n",
    "                        article_url TEXT,\n",
    "                        source_name TEXT,\n",
    "                        published_date TEXT,\n",
    "                        relevance_score REAL DEFAULT 0.0,\n",
    "                        cached_date TEXT NOT NULL,\n",
    "                        expires_date TEXT NOT NULL,\n",
    "                        FOREIGN KEY (company_symbol) REFERENCES companies (symbol)\n",
    "                    )\n",
    "                ''')\n",
    "                \n",
    "                # Create indexes for performance\n",
    "                cursor.execute('''\n",
    "                    CREATE INDEX IF NOT EXISTS idx_news_symbol_date \n",
    "                    ON news_cache(company_symbol, expires_date)\n",
    "                ''')\n",
    "                \n",
    "                cursor.execute('''\n",
    "                    CREATE INDEX IF NOT EXISTS idx_news_published_date \n",
    "                    ON news_cache(published_date DESC)\n",
    "                ''')\n",
    "                \n",
    "                conn.commit()\n",
    "                logger.info(\"News cache tables created successfully\")\n",
    "                \n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Database error creating news tables: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def collect_company_news(self, company_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main orchestrator function that collects news for a company\n",
    "        \n",
    "        Args:\n",
    "            company_data: Output from user_input_processor.py containing:\n",
    "                - symbol: Company ticker symbol\n",
    "                - company_name: Full company name\n",
    "                - search_terms: List of search keywords\n",
    "                - validated: Boolean confirmation\n",
    "                - Other metadata\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing news articles ready for downstream processing\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        company_symbol = company_data.get('symbol', '').upper()\n",
    "        company_name = company_data.get('company_name', '')\n",
    "        \n",
    "        logger.info(f\"Starting news collection for {company_symbol} ({company_name})\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Check cache first\n",
    "            cached_articles = self.check_cache_first(company_symbol)\n",
    "            if cached_articles:\n",
    "                logger.info(f\"Found {len(cached_articles)} cached articles for {company_symbol}\")\n",
    "                processing_time = time.time() - start_time\n",
    "                return self._format_final_output(company_data, cached_articles, processing_time, from_cache=True)\n",
    "            \n",
    "            # Step 2: Fetch fresh news if cache miss\n",
    "            logger.info(f\"Cache miss for {company_symbol}, fetching fresh news...\")\n",
    "            fresh_articles = self.fetch_fresh_news(company_data)\n",
    "            \n",
    "            # Step 3: Store in cache for future requests\n",
    "            if fresh_articles:\n",
    "                self.store_news_cache(company_symbol, fresh_articles)\n",
    "                logger.info(f\"Cached {len(fresh_articles)} articles for {company_symbol}\")\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            return self._format_final_output(company_data, fresh_articles, processing_time, from_cache=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error collecting news for {company_symbol}: {e}\")\n",
    "            # Return empty result with error info\n",
    "            return {\n",
    "                'company_data': company_data,\n",
    "                'articles': [],\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'processing_time': time.time() - start_time,\n",
    "                'from_cache': False,\n",
    "                'ready_for_processing': False\n",
    "            }\n",
    "    \n",
    "    def check_cache_first(self, company_symbol: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Check if we have valid cached news for the company\n",
    "        \n",
    "        Args:\n",
    "            company_symbol: NSE ticker symbol\n",
    "            \n",
    "        Returns:\n",
    "            List of cached articles if found and valid, None otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                conn.row_factory = sqlite3.Row\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                current_time = datetime.now().isoformat()\n",
    "                \n",
    "                cursor.execute('''\n",
    "                    SELECT * FROM news_cache \n",
    "                    WHERE company_symbol = ? \n",
    "                    AND expires_date > ?\n",
    "                    ORDER BY published_date DESC\n",
    "                ''', (company_symbol, current_time))\n",
    "                \n",
    "                cached_rows = cursor.fetchall()\n",
    "                \n",
    "                if not cached_rows:\n",
    "                    return None\n",
    "                \n",
    "                # Convert to list of dictionaries, normalized to RSS schema\n",
    "                # Expected by downstream filtering: title, description, link, published, source\n",
    "                articles = []\n",
    "                for row in cached_rows:\n",
    "                    articles.append({\n",
    "                        'title': row['article_title'],\n",
    "                        'description': row['article_content'] or '',\n",
    "                        'link': row['article_url'] or '',\n",
    "                        'source': row['source_name'] or '',\n",
    "                        'published': row['published_date'] or '',\n",
    "                        'relevance_score': row['relevance_score']\n",
    "                    })\n",
    "                \n",
    "                logger.info(f\"Found {len(articles)} cached articles for {company_symbol}\")\n",
    "                return articles\n",
    "                \n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Database error checking cache for {company_symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def fetch_fresh_news(self, company_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch fresh news from RSS sources and Gemini LLM when cache miss occurs.\n",
    "        \"\"\"\n",
    "        company_symbol = company_data.get('symbol', '')\n",
    "        company_name = company_data.get('company_name', '')\n",
    "        search_terms = company_data.get('search_terms', [company_name])\n",
    "        \n",
    "        logger.info(f\"Fetching fresh news for {company_symbol}\")\n",
    "    \n",
    "        # Check if RSS manager is available\n",
    "        if not self.rss_manager:\n",
    "            logger.error(\"RSS Manager not initialized\")\n",
    "            return []\n",
    "    \n",
    "        try:\n",
    "            # Fetch from RSS sources using RSS Manager\n",
    "            rss_result = self.rss_manager.fetch_all_rss_feeds()\n",
    "            raw_articles = rss_result.get('articles', []) if rss_result.get('success', False) else []\n",
    "            logger.info(f\"Fetched {len(raw_articles)} raw articles from RSS sources\")\n",
    "    \n",
    "            # Map RSS field names to expected format\n",
    "            rss_articles = self.map_rss_articles_to_expected_format(raw_articles)\n",
    "            filtered_rss = self._filter_articles_basic(rss_articles, company_data)\n",
    "    \n",
    "            # Deduplicate RSS internally\n",
    "            unique_rss = {}\n",
    "            for art in filtered_rss:\n",
    "                key = (art['title'].lower(), art['link'])\n",
    "                if key not in unique_rss:\n",
    "                    unique_rss[key] = art\n",
    "            filtered_rss = list(unique_rss.values())[:self.max_articles_per_company]\n",
    "            for art in filtered_rss:\n",
    "                art['weight'] = 1.0\n",
    "    \n",
    "            all_articles = filtered_rss\n",
    "    \n",
    "            # Fetch from Gemini LLM Searcher (MVP)\n",
    "            try:\n",
    "                llm_articles = self.llm_searcher.search_news(company_name, company_symbol)\n",
    "                if llm_articles:\n",
    "                    logger.info(f\"Gemini fetched {len(llm_articles)} articles\")\n",
    "                    # Convert Gemini articles into RSS-like structure for consistency\n",
    "                    for art in llm_articles:\n",
    "                        all_articles.append({\n",
    "                            'title': art['title'],\n",
    "                            'description': art['summary'],\n",
    "                            'link': art['url'],\n",
    "                            'published': art['published'].isoformat(),\n",
    "                            'source': art['source'],\n",
    "                            'relevance_score': art['relevance_score'],\n",
    "                            'weight': art['weight']\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Gemini LLM fetch failed: {e}\")\n",
    "    \n",
    "            # Deduplicate (Gemini > RSS priority)\n",
    "            deduped_articles = self._deduplicate_articles(all_articles)\n",
    "    \n",
    "            logger.info(f\"Found {len(deduped_articles)} total articles after merging RSS + Gemini\")\n",
    "            return deduped_articles\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching fresh news for {company_symbol}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def map_rss_articles_to_expected_format(self, raw_articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        mapped_articles = []\n",
    "        for article in raw_articles:\n",
    "            try:\n",
    "                mapped_article = {\n",
    "                    'title': article.get('title', ''),\n",
    "                    'description': article.get('description', ''),  # Keep original\n",
    "                    'link': article.get('link', ''),               # Keep original  \n",
    "                    'published': article.get('published', ''),     # Keep original\n",
    "                    'source': article.get('source', ''),\n",
    "                    'author': article.get('author', ''),\n",
    "                    'relevance_score': 0.0\n",
    "                }\n",
    "                if mapped_article['title'] and mapped_article['link']:\n",
    "                    mapped_articles.append(mapped_article)\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error mapping article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return mapped_articles\n",
    "\n",
    "    def _filter_articles_basic(self, articles, company_data):\n",
    "        \"\"\"Fixed basic filtering with better matching logic\"\"\"\n",
    "        company_terms = company_data.get(\"search_terms\", [])\n",
    "        company_name = company_data.get('company_name', '')\n",
    "        company_symbol = company_data.get('symbol', '')\n",
    "        \n",
    "        # Normalize all search terms to lowercase\n",
    "        search_terms = []\n",
    "        \n",
    "        # Add search terms from company_data\n",
    "        if company_terms:\n",
    "            search_terms.extend([term.lower() for term in company_terms if isinstance(term, str)])\n",
    "        \n",
    "        # Add company name and variations\n",
    "        if company_name:\n",
    "            search_terms.append(company_name.lower())\n",
    "            # Add company name without common suffixes\n",
    "            clean_name = self._clean_company_name(company_name).lower()\n",
    "            if clean_name and clean_name != company_name.lower():\n",
    "                search_terms.append(clean_name)\n",
    "        \n",
    "        # Add symbol\n",
    "        if company_symbol:\n",
    "            search_terms.append(company_symbol.lower())\n",
    "        \n",
    "        # Remove duplicates\n",
    "        search_terms = list(set(search_terms))\n",
    "        \n",
    "        logger.info(f\"Filtering with search terms: {search_terms}\")\n",
    "        \n",
    "        relevant_articles = []\n",
    "        \n",
    "        for article in articles:\n",
    "            title = article.get(\"title\", \"\").lower()\n",
    "            description = article.get(\"description\", \"\").lower()\n",
    "            article_text = f\"{title} {description}\"\n",
    "            \n",
    "            is_relevant = False\n",
    "            matched_terms = []\n",
    "            \n",
    "            # Check for matches with any search term\n",
    "            for term in search_terms:\n",
    "                if term in article_text:\n",
    "                    is_relevant = True\n",
    "                    matched_terms.append(term)\n",
    "                    break\n",
    "                \n",
    "                # Also check for partial matches (words within the term)\n",
    "                term_words = term.split()\n",
    "                if len(term_words) > 1:\n",
    "                    # Check if all words of the term appear in the article\n",
    "                    if all(word in article_text for word in term_words):\n",
    "                        is_relevant = True\n",
    "                        matched_terms.append(term)\n",
    "                        break\n",
    "            \n",
    "            if is_relevant:\n",
    "                article[\"relevance_score\"] = 0.7\n",
    "                article[\"matched_terms\"] = matched_terms\n",
    "                relevant_articles.append(article)\n",
    "                logger.debug(f\"Matched article: {title[:60]}... (terms: {matched_terms})\")\n",
    "        \n",
    "        logger.info(f\"Found {len(relevant_articles)} relevant articles out of {len(articles)} total\")\n",
    "        return relevant_articles\n",
    "\n",
    "    def _deduplicate_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Deduplicate articles by URL. Gemini articles take priority over RSS.\n",
    "        \"\"\"\n",
    "        seen_urls = set()\n",
    "        deduped = []\n",
    "    \n",
    "        # Sort so that Gemini articles come first\n",
    "        sorted_articles = sorted(articles, key=lambda a: 0 if a.get(\"source\") == \"gemini\" else 1)\n",
    "    \n",
    "        for art in sorted_articles:\n",
    "            url = art.get('link') or art.get('url')\n",
    "            if url and url not in seen_urls:\n",
    "                seen_urls.add(url)\n",
    "                deduped.append(art)\n",
    "    \n",
    "        return deduped\n",
    "\n",
    "    def _clean_company_name(self, company_name: str) -> str:\n",
    "        \"\"\"Remove common company suffixes - this method is missing from news_collector.py\"\"\"\n",
    "        if not company_name:\n",
    "            return \"\"\n",
    "        \n",
    "        clean_name = company_name.strip()\n",
    "        suffixes = [' Limited', ' Ltd', ' Pvt', ' Private', ' Company', ' Corp', ' Corporation', ' Inc']\n",
    "        \n",
    "        for suffix in suffixes:\n",
    "            if clean_name.endswith(suffix):\n",
    "                clean_name = clean_name[:-len(suffix)].strip()\n",
    "                break\n",
    "        \n",
    "        return clean_name\n",
    "\n",
    "    def store_news_cache(self, company_symbol: str, articles: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Store fetched articles in cache for future requests\n",
    "        \n",
    "        Args:\n",
    "            company_symbol: NSE ticker symbol\n",
    "            articles: List of articles to cache\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                cached_date = datetime.now().isoformat()\n",
    "                expires_date = (datetime.now() + timedelta(days=self.cache_expiry_days)).isoformat()\n",
    "                \n",
    "                # Clear existing cache for this company first\n",
    "                cursor.execute('DELETE FROM news_cache WHERE company_symbol = ?', (company_symbol,))\n",
    "                \n",
    "                # Insert new articles (map from RSS schema to DB columns)\n",
    "                for article in articles:\n",
    "                    cursor.execute('''\n",
    "                        INSERT INTO news_cache \n",
    "                        (company_symbol, article_title, article_content, article_url, \n",
    "                         source_name, published_date, relevance_score, cached_date, expires_date)\n",
    "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ''', (\n",
    "                        company_symbol,\n",
    "                        article.get('title', ''),\n",
    "                        article.get('description', ''),\n",
    "                        article.get('link', ''),\n",
    "                        article.get('source', ''),\n",
    "                        article.get('published', ''),\n",
    "                        article.get('relevance_score', 0.0),\n",
    "                        cached_date,\n",
    "                        expires_date\n",
    "                    ))\n",
    "                \n",
    "                conn.commit()\n",
    "                logger.info(f\"Cached {len(articles)} articles for {company_symbol}\")\n",
    "                \n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Database error storing cache for {company_symbol}: {e}\")\n",
    "    \n",
    "    def _format_final_output(self, company_data: Dict[str, Any], articles: List[Dict[str, Any]], \n",
    "                           processing_time: float, from_cache: bool) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Format the final output for downstream processing\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'company_data': company_data,\n",
    "            'articles': articles,\n",
    "            'article_count': len(articles),\n",
    "            'success': len(articles) >= self.min_articles_required,\n",
    "            'processing_time': round(processing_time, 2),\n",
    "            'from_cache': from_cache,\n",
    "            'cache_expiry_days': self.cache_expiry_days,\n",
    "            'ready_for_processing': len(articles) > 0,\n",
    "            'metadata': {\n",
    "                'collection_timestamp': datetime.now().isoformat(),\n",
    "                'min_articles_required': self.min_articles_required,\n",
    "                'max_articles_limit': self.max_articles_per_company\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def cleanup_expired_cache(self):\n",
    "        \"\"\"\n",
    "        Background cleanup job to remove expired cache entries\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                current_time = datetime.now().isoformat()\n",
    "                \n",
    "                # Delete expired entries\n",
    "                cursor.execute('DELETE FROM news_cache WHERE expires_date < ?', (current_time,))\n",
    "                deleted_count = cursor.rowcount\n",
    "                \n",
    "                conn.commit()\n",
    "                \n",
    "                if deleted_count > 0:\n",
    "                    logger.info(f\"Cleaned up {deleted_count} expired cache entries\")\n",
    "                \n",
    "                return deleted_count\n",
    "                \n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Database error during cache cleanup: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get statistics about cached news data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                # Total cached articles\n",
    "                cursor.execute('SELECT COUNT(*) FROM news_cache')\n",
    "                total_articles = cursor.fetchone()[0]\n",
    "                \n",
    "                # Unique companies with cache\n",
    "                cursor.execute('SELECT COUNT(DISTINCT company_symbol) FROM news_cache')\n",
    "                unique_companies = cursor.fetchone()[0]\n",
    "                \n",
    "                # Articles by source\n",
    "                cursor.execute('''\n",
    "                    SELECT source_name, COUNT(*) as count \n",
    "                    FROM news_cache \n",
    "                    GROUP BY source_name \n",
    "                    ORDER BY count DESC\n",
    "                ''')\n",
    "                source_stats = dict(cursor.fetchall())\n",
    "                \n",
    "                return {\n",
    "                    'total_cached_articles': total_articles,\n",
    "                    'companies_with_cache': unique_companies,\n",
    "                    'articles_by_source': source_stats,\n",
    "                    'cache_expiry_days': self.cache_expiry_days\n",
    "                }\n",
    "                \n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Database error getting cache stats: {e}\")\n",
    "            return {'error': str(e)}\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\": \n",
    "    # Initialize news collector\n",
    "    collector = NewsCollector()\n",
    "    \n",
    "    # Example input from user_input_processor.py\n",
    "    sample_company_data = {\n",
    "        \"symbol\": \"ADANIPORTS\",\n",
    "        \"company_name\": \"Adani Ports and Special Economic Zone Limited\",\n",
    "        \"search_terms\": [\"ADANIPORTS\", \"Adani Ports\", \"Adani SEZ\"],\n",
    "        \"series\": \"EQ\",\n",
    "        \"isin\": \"INE742F01042\",\n",
    "        \"validated\": True,\n",
    "        \"timestamp\": \"2025-09-22T09:45:00Z\",\n",
    "        \"source\": \"nse_database\",\n",
    "        \"ready_for_news_scraper\": True\n",
    "    }\n",
    "    \n",
    "    print(\"Testing News Collector with RSS Manager Integration...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test news collection\n",
    "    result = collector.collect_company_news(sample_company_data)\n",
    "    \n",
    "    print(f\"Collection Success: {result['success']}\")\n",
    "    print(f\"Articles Found: {result['article_count']}\")\n",
    "    print(f\"Processing Time: {result['processing_time']} seconds\")\n",
    "    print(f\"From Cache: {result['from_cache']}\")\n",
    "    print(f\"Ready for Processing: {result['ready_for_processing']}\")\n",
    "    \n",
    "    # Show sample articles if found\n",
    "    if result.get('articles'):\n",
    "        print(f\"\\nSample Articles:\")\n",
    "        for i, article in enumerate(result['articles'][:3], 1):\n",
    "            print(f\"{i}. {article.get('title', 'No Title')[:80]}...\")\n",
    "            print(f\"   Source: {article.get('source', 'Unknown')}\")\n",
    "            print(f\"   Relevance: {article.get('relevance_score', 0.0)}\")\n",
    "    \n",
    "    # Test cache stats\n",
    "    print(\"\\nCache Statistics:\")\n",
    "    stats = collector.get_cache_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Test cache cleanup\n",
    "    print(f\"\\nCleanup Result: {collector.cleanup_expired_cache()} entries removed\")\n",
    "    \n",
    "    print(\"\\nNews Collector with RSS Manager integration testing completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
