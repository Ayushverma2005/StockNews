{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cbb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Optional, Any\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(raw_html: str) -> str:\n",
    "    if not raw_html:\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    # remove scripts and styles\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "   \n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RSSManager:\n",
    "    \"\"\"RSS feed manager for Indian financial news sources\"\"\"\n",
    "    \n",
    "    def __init__(self, request_timeout=10, max_workers=3, retry_attempts=2):\n",
    "        self.request_timeout = request_timeout\n",
    "        self.max_workers = max_workers\n",
    "        self.retry_attempts = retry_attempts\n",
    "        \n",
    "        # RSS source configuration\n",
    "        self.rss_sources = self.get_rss_source_config()\n",
    "        \n",
    "        # Request session for connection pooling\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'application/rss+xml, application/xml, text/xml, */*',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Connection': 'keep-alive'\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"RSSManager initialized with {len(self.rss_sources)} sources, timeout={request_timeout}s\")\n",
    "    \n",
    "    def get_rss_source_config(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        RSS source configuration with URLs and parsing settings\n",
    "        Returns dictionary of source configurations\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'economic_times': {\n",
    "                'name': 'Economic Times Business',\n",
    "                'url': 'https://economictimes.indiatimes.com/rssfeedstopstories.cms',\n",
    "                'backup_url': 'https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms',\n",
    "                'format': 'xml',\n",
    "                'encoding': 'utf-8',\n",
    "                'priority': 1,\n",
    "                'active': True,\n",
    "                'description_field': 'summary',\n",
    "                'date_field': 'published'\n",
    "            },\n",
    "            #'business_standard': {\n",
    "            #    'name': 'Business Standard',\n",
    "            #    'url': 'https://www.business-standard.com/rss/markets-106.rss',\n",
    "            #    'backup_url': 'https://www.business-standard.com/rss/finance-103.rss',\n",
    "            #    'format': 'xml',\n",
    "            #    'encoding': 'utf-8',\n",
    "            #    'priority': 2,\n",
    "            #    'active': True,\n",
    "            #    'description_field': 'summary',\n",
    "            #    'date_field': 'published'\n",
    "            #},\n",
    "            'livemint': {\n",
    "                'name': 'LiveMint Markets',\n",
    "                'url': 'https://www.livemint.com/rss/markets',\n",
    "                'backup_url': 'https://www.livemint.com/rss/companies',\n",
    "                'format': 'xml',\n",
    "                'encoding': 'utf-8',\n",
    "                'priority': 3,\n",
    "                'active': True,\n",
    "                'description_field': 'summary',\n",
    "                'date_field': 'published'\n",
    "            },\n",
    "            'hindu_businessline': {\n",
    "                'name': 'Hindu BusinessLine',\n",
    "                'url': 'https://www.thehindubusinessline.com/markets/stock-markets/feeder/default.rss',\n",
    "                'backup_url': 'https://www.thehindubusinessline.com/companies/feeder/default.rss',\n",
    "                'format': 'xml',\n",
    "                'encoding': 'utf-8',\n",
    "                'priority': 4,\n",
    "                'active': True,\n",
    "                'description_field': 'summary',\n",
    "                'date_field': 'published'\n",
    "            },\n",
    "            #'financial_express': {\n",
    "            #    'name': 'Financial Express',\n",
    "            #    'url': 'https://www.financialexpress.com/market/rss/',\n",
    "            #    'backup_url': 'https://www.financialexpress.com/industry/rss/',\n",
    "            #    'format': 'xml',\n",
    "            #    'encoding': 'utf-8',\n",
    "            #    'priority': 5,\n",
    "            #    'active': True,\n",
    "            #    'description_field': 'summary',\n",
    "            #    'date_field': 'published'\n",
    "            #},\n",
    "            'ndtv_business': {\n",
    "                'name': 'NDTV Business',\n",
    "                'url': 'https://feeds.feedburner.com/ndtvprofit-latest',\n",
    "                'backup_url': 'https://www.ndtv.com/business/rss',\n",
    "                'format': 'xml',\n",
    "                'encoding': 'utf-8',\n",
    "                'priority': 6,\n",
    "                'active': True,\n",
    "                'description_field': 'summary',\n",
    "                'date_field': 'published'\n",
    "            },\n",
    "            'zee_business': {\n",
    "                'name': 'Zee Business',\n",
    "                'url': 'https://zeenews.india.com/rss/business.xml',\n",
    "                'backup_url': 'https://zeenews.india.com/rss/stock-market.xml',\n",
    "                'format': 'xml',\n",
    "                'encoding': 'utf-8',\n",
    "                'priority': 7,\n",
    "                'active': True,\n",
    "                'description_field': 'summary',\n",
    "                'date_field': 'published'\n",
    "            },\n",
    "            'moneycontrol': {\n",
    "                'name': 'Moneycontrol Markets',\n",
    "                'url': 'https://www.moneycontrol.com/rss/business.xml',\n",
    "                'backup_url': 'https://www.moneycontrol.com/rss/marketsnews.xml',\n",
    "                'format': 'xml',\n",
    "                'encoding': 'utf-8',\n",
    "                'priority': 8,\n",
    "                'active': False,  # Disabled due to 403 errors\n",
    "                'description_field': 'summary',\n",
    "                'date_field': 'published'\n",
    "            },\n",
    "            'news18_business': {\n",
    "                'name': 'News18 Business',\n",
    "                'url': 'https://www.news18.com/rss/business.xml',\n",
    "                'backup_url': 'https://www.news18.com/rss/india.xml',\n",
    "                'format': 'xml',\n",
    "                'encoding': 'utf-8',\n",
    "                'priority': 9,\n",
    "                'active': True,\n",
    "                'description_field': 'summary',\n",
    "                'date_field': 'published'\n",
    "            },\n",
    "            #'outlook_money': {\n",
    "            #    'name': 'Outlook Money',\n",
    "            #    'url': 'https://www.outlookindia.com/rss/business',\n",
    "            #    'backup_url': 'https://www.outlookindia.com/business/rss',\n",
    "            #    'format': 'xml',\n",
    "            #    'encoding': 'utf-8',\n",
    "            #    'priority': 10,\n",
    "            #    'active': True,\n",
    "            #    'description_field': 'summary',\n",
    "            #    'date_field': 'published'\n",
    "            #}\n",
    "        }\n",
    "    \n",
    "    def fetch_all_rss_feeds(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main function: Parallel fetch from all RSS sources\n",
    "        Returns combined results with success/failure tracking\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting parallel RSS fetch from all sources\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Filter active sources\n",
    "        active_sources = {\n",
    "            source_id: config for source_id, config in self.rss_sources.items()\n",
    "            if config.get('active', True)\n",
    "        }\n",
    "        \n",
    "        if not active_sources:\n",
    "            logger.error(\"No active RSS sources configured\")\n",
    "            return self._create_empty_result(\"No active sources\")\n",
    "        \n",
    "        # Parallel execution\n",
    "        results = {}\n",
    "        failed_sources = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit all fetch tasks\n",
    "            future_to_source = {\n",
    "                executor.submit(self._fetch_single_rss_feed, source_id, config): source_id\n",
    "                for source_id, config in active_sources.items()\n",
    "            }\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(future_to_source, timeout=self.request_timeout + 5):\n",
    "                source_id = future_to_source[future]\n",
    "                \n",
    "                try:\n",
    "                    source_result = future.result()\n",
    "                    \n",
    "                    if source_result.get('success', False):\n",
    "                        results[source_id] = source_result\n",
    "                        logger.info(f\"Successfully fetched {len(source_result.get('articles', []))} articles from {source_id}\")\n",
    "                    else:\n",
    "                        failed_sources.append(source_id)\n",
    "                        logger.warning(f\"Failed to fetch from {source_id}: {source_result.get('error', 'Unknown error')}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed_sources.append(source_id)\n",
    "                    logger.error(f\"Exception fetching from {source_id}: {str(e)}\")\n",
    "        \n",
    "        # Handle failures and compile final results\n",
    "        final_result = self._compile_fetch_results(results, failed_sources)\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        final_result['execution_time_seconds'] = round(execution_time, 2)\n",
    "        \n",
    "        logger.info(f\"RSS fetch completed in {execution_time:.2f}s. Success: {len(results)}/{len(active_sources)} sources\")\n",
    "        \n",
    "        # Handle failures if any\n",
    "        if failed_sources:\n",
    "            failure_result = self.handle_rss_failures(failed_sources)\n",
    "            final_result['failure_handling'] = failure_result\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    def _fetch_single_rss_feed(self, source_id: str, source_config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch RSS feed from a single source with retry logic\n",
    "        \"\"\"\n",
    "        source_name = source_config.get('name', source_id)\n",
    "        primary_url = source_config.get('url')\n",
    "        backup_url = source_config.get('backup_url')\n",
    "        \n",
    "        # Try primary URL first\n",
    "        urls_to_try = [primary_url]\n",
    "        if backup_url:\n",
    "            urls_to_try.append(backup_url)\n",
    "        \n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(self.retry_attempts):\n",
    "            for url_index, url in enumerate(urls_to_try):\n",
    "                try:\n",
    "                    logger.debug(f\"Fetching {source_name} from {url} (attempt {attempt + 1})\")\n",
    "                    \n",
    "                    response = self.session.get(\n",
    "                        url,\n",
    "                        timeout=self.request_timeout,\n",
    "                        allow_redirects=True\n",
    "                    )\n",
    "                    \n",
    "                    response.raise_for_status()\n",
    "                    \n",
    "                    # Parse the RSS content\n",
    "                    articles = self.parse_rss_content(response.content, source_config)\n",
    "                    \n",
    "                    if articles:\n",
    "                        return {\n",
    "                            'success': True,\n",
    "                            'source_id': source_id,\n",
    "                            'source_name': source_name,\n",
    "                            'articles': articles,\n",
    "                            'url_used': url,\n",
    "                            'is_backup_url': url_index > 0,\n",
    "                            'fetch_timestamp': datetime.now(timezone.utc).isoformat(),\n",
    "                            'article_count': len(articles)\n",
    "                        }\n",
    "                    else:\n",
    "                        last_error = \"No articles parsed from RSS feed\"\n",
    "                        \n",
    "                except requests.exceptions.Timeout:\n",
    "                    last_error = f\"Timeout after {self.request_timeout}s\"\n",
    "                    logger.warning(f\"{source_name} timeout on attempt {attempt + 1}\")\n",
    "                    \n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    last_error = f\"Request error: {str(e)}\"\n",
    "                    logger.warning(f\"{source_name} request error: {str(e)}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    last_error = f\"Parsing error: {str(e)}\"\n",
    "                    logger.error(f\"{source_name} parsing error: {str(e)}\")\n",
    "            \n",
    "            # Wait before retry (exponential backoff)\n",
    "            if attempt < self.retry_attempts - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        # All attempts failed\n",
    "        return {\n",
    "            'success': False,\n",
    "            'source_id': source_id,\n",
    "            'source_name': source_name,\n",
    "            'error': last_error or \"Unknown error\",\n",
    "            'attempts': self.retry_attempts,\n",
    "            'urls_tried': urls_to_try\n",
    "        }\n",
    "    \n",
    "    def parse_rss_content(self, rss_content: bytes, source_config: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract articles from RSS XML/JSON content\n",
    "        Returns list of standardized article dictionaries\n",
    "        \"\"\"\n",
    "        try:\n",
    "            source_name = source_config.get('name', 'Unknown')\n",
    "            \n",
    "            # Use feedparser for robust RSS/Atom parsing\n",
    "            feed = feedparser.parse(rss_content)\n",
    "            \n",
    "            if feed.bozo and feed.bozo_exception:\n",
    "                logger.warning(f\"RSS parsing warning for {source_name}: {feed.bozo_exception}\")\n",
    "            \n",
    "            if not hasattr(feed, 'entries') or not feed.entries:\n",
    "                logger.warning(f\"No entries found in RSS feed for {source_name}\")\n",
    "                return []\n",
    "            \n",
    "            articles = []\n",
    "            \n",
    "            for entry in feed.entries:\n",
    "                try:\n",
    "                    article = self._extract_article_data(entry, source_config)\n",
    "                    if article:\n",
    "                        articles.append(article)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error parsing individual article from {source_name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.debug(f\"Parsed {len(articles)} articles from {source_name}\")\n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing RSS content: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_article_data(self, entry: Any, source_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract standardized article data from RSS entry\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract basic fields\n",
    "            title = self._safe_get_text(entry, 'title', '').strip()\n",
    "            link = self._safe_get_text(entry, 'link', '').strip()\n",
    "            \n",
    "            if not title or not link:\n",
    "                return None\n",
    "    \n",
    "            # Extract description/summary\n",
    "            description_field = source_config.get('description_field', 'summary')\n",
    "            description = self._safe_get_text(entry, description_field, '')\n",
    "    \n",
    "            # Fallback description fields\n",
    "            if not description:\n",
    "                for field in ['content_encoded', 'summary', 'description', 'content']:\n",
    "                    description = self._safe_get_text(entry, field, '')\n",
    "                    if description:\n",
    "                        break\n",
    "    \n",
    "            # Clean description HTML (always run, even if already plain)\n",
    "            description = clean_html(description)\n",
    "    \n",
    "            # Extract and parse date\n",
    "            date_field = source_config.get('date_field', 'published')\n",
    "            published_date = self._extract_publish_date(entry, date_field)\n",
    "    \n",
    "            # Extract author if available\n",
    "            author = self._safe_get_text(entry, 'author', '')\n",
    "    \n",
    "            # Clean title too\n",
    "            title = clean_html(title)\n",
    "    \n",
    "            # Create standardized article object\n",
    "            article = {\n",
    "                'title': title,\n",
    "                'description': description,\n",
    "                'link': link,\n",
    "                'published': published_date,\n",
    "                'author': author,\n",
    "                'source': source_config.get('name', 'Unknown'),\n",
    "                'source_id': source_config.get('source_id', ''),\n",
    "                \n",
    "                # Additional metadata\n",
    "                'guid': self._safe_get_text(entry, 'guid', ''),\n",
    "                'category': self._extract_categories(entry),\n",
    "                'content_length': len(description),\n",
    "                'extraction_timestamp': datetime.now(timezone.utc).isoformat(),\n",
    "                \n",
    "                # Raw entry for debugging (optional)\n",
    "                'raw_entry_keys': list(entry.keys()) if hasattr(entry, 'keys') else []\n",
    "            }\n",
    "            \n",
    "            return article\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error extracting article data: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _safe_get_text(self, entry: Any, field: str, default: str = '') -> str:\n",
    "        \"\"\"\n",
    "        Safely extract text content from RSS entry field\n",
    "        \"\"\"\n",
    "        try:\n",
    "            value = getattr(entry, field, None)\n",
    "            \n",
    "            if value is None:\n",
    "                return default\n",
    "            \n",
    "            # Handle different content formats\n",
    "            if isinstance(value, str):\n",
    "                return value\n",
    "            elif isinstance(value, list) and value:\n",
    "                return str(value[0])\n",
    "            elif hasattr(value, 'value'):\n",
    "                return str(value.value)\n",
    "            elif hasattr(value, 'text'):\n",
    "                return str(value.text)\n",
    "            else:\n",
    "                return str(value)\n",
    "                \n",
    "        except Exception:\n",
    "            return default\n",
    "    \n",
    "    def _extract_publish_date(self, entry: Any, date_field: str = 'published') -> str:\n",
    "        \"\"\"\n",
    "        Extract and standardize publication date\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try different date fields\n",
    "            date_fields = [date_field, 'published', 'updated', 'pubDate']\n",
    "            \n",
    "            for field in date_fields:\n",
    "                date_value = getattr(entry, field, None)\n",
    "                \n",
    "                if date_value:\n",
    "                    # Handle feedparser's parsed time\n",
    "                    if hasattr(entry, f\"{field}_parsed\") and getattr(entry, f\"{field}_parsed\"):\n",
    "                        parsed_time = getattr(entry, f\"{field}_parsed\")\n",
    "                        if parsed_time:\n",
    "                            dt = datetime(*parsed_time[:6], tzinfo=timezone.utc)\n",
    "                            return dt.isoformat()\n",
    "                    \n",
    "                    # Handle string dates\n",
    "                    if isinstance(date_value, str):\n",
    "                        return date_value\n",
    "            \n",
    "            # Default to current time if no date found\n",
    "            return datetime.now(timezone.utc).isoformat()\n",
    "            \n",
    "        except Exception:\n",
    "            return datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    def _extract_categories(self, entry: Any) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract categories/tags from RSS entry\n",
    "        \"\"\"\n",
    "        try:\n",
    "            categories = []\n",
    "            \n",
    "            # Check tags field\n",
    "            if hasattr(entry, 'tags') and entry.tags:\n",
    "                for tag in entry.tags:\n",
    "                    if hasattr(tag, 'term'):\n",
    "                        categories.append(tag.term)\n",
    "            \n",
    "            # Check category field\n",
    "            if hasattr(entry, 'category'):\n",
    "                if isinstance(entry.category, str):\n",
    "                    categories.append(entry.category)\n",
    "                elif isinstance(entry.category, list):\n",
    "                    categories.extend(entry.category)\n",
    "            \n",
    "            return categories\n",
    "            \n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    def _compile_fetch_results(self, successful_results: Dict[str, Any], failed_sources: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compile results from all RSS sources into final response\n",
    "        \"\"\"\n",
    "        all_articles = []\n",
    "        source_stats = {}\n",
    "        \n",
    "        # Collect articles from successful sources\n",
    "        for source_id, result in successful_results.items():\n",
    "            articles = result.get('articles', [])\n",
    "            all_articles.extend(articles)\n",
    "            \n",
    "            source_stats[source_id] = {\n",
    "                'source_name': result.get('source_name', source_id),\n",
    "                'article_count': len(articles),\n",
    "                'success': True,\n",
    "                'url_used': result.get('url_used', ''),\n",
    "                'is_backup_url': result.get('is_backup_url', False)\n",
    "            }\n",
    "        \n",
    "        # Add failed source stats\n",
    "        for source_id in failed_sources:\n",
    "            source_config = self.rss_sources.get(source_id, {})\n",
    "            source_stats[source_id] = {\n",
    "                'source_name': source_config.get('name', source_id),\n",
    "                'article_count': 0,\n",
    "                'success': False,\n",
    "                'error': 'Failed to fetch'\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'success': len(successful_results) > 0,\n",
    "            'total_articles': len(all_articles),\n",
    "            'articles': all_articles,\n",
    "            'sources_attempted': len(self.rss_sources),\n",
    "            'sources_successful': len(successful_results),\n",
    "            'sources_failed': len(failed_sources),\n",
    "            'source_stats': source_stats,\n",
    "            'fetch_timestamp': datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "    \n",
    "    def handle_rss_failures(self, failed_sources: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Handle RSS source failures - continue with available sources\n",
    "        Returns failure analysis and recommendations\n",
    "        \"\"\"\n",
    "        logger.info(f\"Handling failures for {len(failed_sources)} sources: {failed_sources}\")\n",
    "        \n",
    "        total_sources = len(self.rss_sources)\n",
    "        successful_sources = total_sources - len(failed_sources)\n",
    "        \n",
    "        # Determine severity\n",
    "        if successful_sources == 0:\n",
    "            severity = 'critical'\n",
    "            recommendation = 'No RSS sources available. Check network connection and source URLs.'\n",
    "        elif successful_sources < total_sources / 2:\n",
    "            severity = 'high'\n",
    "            recommendation = 'More than half of RSS sources failed. Content may be limited.'\n",
    "        else:\n",
    "            severity = 'low'\n",
    "            recommendation = 'Some RSS sources failed but majority are working. Content should be sufficient.'\n",
    "        \n",
    "        failure_analysis = {\n",
    "            'severity': severity,\n",
    "            'failed_sources': failed_sources,\n",
    "            'successful_sources': successful_sources,\n",
    "            'total_sources': total_sources,\n",
    "            'success_rate': round(successful_sources / total_sources * 100, 1),\n",
    "            'recommendation': recommendation,\n",
    "            'failed_source_details': []\n",
    "        }\n",
    "        \n",
    "        # Add details for each failed source\n",
    "        for source_id in failed_sources:\n",
    "            source_config = self.rss_sources.get(source_id, {})\n",
    "            failure_analysis['failed_source_details'].append({\n",
    "                'source_id': source_id,\n",
    "                'source_name': source_config.get('name', source_id),\n",
    "                'primary_url': source_config.get('url', ''),\n",
    "                'backup_url': source_config.get('backup_url', ''),\n",
    "                'priority': source_config.get('priority', 99)\n",
    "            })\n",
    "        \n",
    "        return failure_analysis\n",
    "    \n",
    "    def _create_empty_result(self, reason: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create empty result structure for error cases\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'success': False,\n",
    "            'total_articles': 0,\n",
    "            'articles': [],\n",
    "            'sources_attempted': 0,\n",
    "            'sources_successful': 0,\n",
    "            'sources_failed': 0,\n",
    "            'error': reason,\n",
    "            'fetch_timestamp': datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "    \n",
    "    def test_single_source(self, source_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Test a single RSS source for debugging\n",
    "        \"\"\"\n",
    "        if source_id not in self.rss_sources:\n",
    "            return {'error': f'Source {source_id} not configured'}\n",
    "        \n",
    "        source_config = self.rss_sources[source_id]\n",
    "        logger.info(f\"Testing RSS source: {source_id}\")\n",
    "        \n",
    "        return self._fetch_single_rss_feed(source_id, source_config)\n",
    "    \n",
    "    def get_source_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get status of all configured RSS sources\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'total_sources': len(self.rss_sources),\n",
    "            'active_sources': len([s for s in self.rss_sources.values() if s.get('active', True)]),\n",
    "            'sources': {\n",
    "                source_id: {\n",
    "                    'name': config.get('name', source_id),\n",
    "                    'url': config.get('url', ''),\n",
    "                    'active': config.get('active', True),\n",
    "                    'priority': config.get('priority', 99)\n",
    "                }\n",
    "                for source_id, config in self.rss_sources.items()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up resources\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'session'):\n",
    "            self.session.close()\n",
    "        logger.info(\"RSSManager closed\")\n",
    "\n",
    "# Convenience function for quick usage\n",
    "def create_rss_manager(timeout=10, max_workers=3, retry_attempts=2) -> RSSManager:\n",
    "    \"\"\"Factory function to create RSSManager instance\"\"\"\n",
    "    return RSSManager(timeout, max_workers, retry_attempts)\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize RSS manager\n",
    "        rss_manager = RSSManager()\n",
    "        \n",
    "        print(\"=== RSS Manager Test ===\")\n",
    "        \n",
    "        # Test source configuration\n",
    "        print(\"\\n1. Source Configuration:\")\n",
    "        status = rss_manager.get_source_status()\n",
    "        print(json.dumps(status, indent=2))\n",
    "        \n",
    "        # Test single source\n",
    "        print(\"\\n2. Testing Single Source (Economic Times):\")\n",
    "        single_test = rss_manager.test_single_source('economic_times')\n",
    "        if single_test.get('success'):\n",
    "            print(f\"   Success: {single_test.get('article_count', 0)} articles\")\n",
    "            if single_test.get('articles'):\n",
    "                print(f\"   Sample: {single_test['articles'][0].get('title', 'N/A')[:80]}...\")\n",
    "        else:\n",
    "            print(f\"   Failed: {single_test.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        # Test parallel fetch from all sources\n",
    "        print(\"\\n3. Parallel Fetch Test:\")\n",
    "        all_results = rss_manager.fetch_all_rss_feeds()\n",
    "        \n",
    "        if all_results.get('success'):\n",
    "            print(f\"   Total Articles: {all_results.get('total_articles', 0)}\")\n",
    "            print(f\"   Successful Sources: {all_results.get('sources_successful', 0)}/{all_results.get('sources_attempted', 0)}\")\n",
    "            print(f\"   Execution Time: {all_results.get('execution_time_seconds', 0)}s\")\n",
    "            \n",
    "            # Show sample articles\n",
    "            articles = all_results.get('articles', [])\n",
    "            if articles:\n",
    "                print(\"\\n   Sample Articles:\")\n",
    "                for i, article in enumerate(articles[:3], 1):\n",
    "                    print(f\"   {i}. {article.get('title', 'N/A')[:60]}...\")\n",
    "                    print(f\"      Source: {article.get('source', 'N/A')}\")\n",
    "                    print(f\"      Published: {article.get('published', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"   Failed: {all_results.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        # Clean up\n",
    "        rss_manager.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "        print(\"Check network connection and RSS URLs.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
