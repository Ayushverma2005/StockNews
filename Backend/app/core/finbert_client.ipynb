{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa31f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finbert_client.py - Local FinBERT Sentiment Analysis Client\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "class FinBERTClient:\n",
    "    \"\"\"\n",
    "    Client for local FinBERT sentiment analysis using PyTorch + Transformers.\n",
    "    Performs batched inference on preprocessed articles and returns sentiment predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 device: str = None,\n",
    "                 batch_size: int = 16):\n",
    "        \"\"\"\n",
    "        Initialize local FinBERT client with model and tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier or local path\n",
    "            device: Device to run inference on ('cuda', 'cpu', or None for auto-detect)\n",
    "            batch_size: Number of texts to process in each batch\n",
    "        \"\"\"\n",
    "         # Resolve backend root → /backend\n",
    "        BASE_DIR = Path(__file__).resolve().parent.parent.parent\n",
    "        MODEL_PATH = BASE_DIR / \"finbert_training/models/finetuned\"\n",
    "        logger.info(f\"Initializing FinBERT client with model: {str(MODEL_PATH)}\")\n",
    "        \n",
    "        # Set device (auto-detect GPU if available)\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # ✅ MINIMAL EDIT: do not load model in __init__\n",
    "        # just store path + lazy-load flags\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self._loaded = False\n",
    "        self.model_name = str(MODEL_PATH)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # FinBERT label mapping: [positive, neutral, negative]\n",
    "        self.label_map = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "    \n",
    "    # ✅ MINIMAL EDIT: lazy-load model only when first used\n",
    "    def _lazy_load(self):\n",
    "        if not self._loaded:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, local_files_only=True)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, local_files_only=True)\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            self._loaded = True\n",
    "            logger.info(\"FinBERT model + tokenizer loaded lazily on first request.\")\n",
    "\n",
    "    def analyze(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Main entry point: Perform sentiment analysis on preprocessed articles.\n",
    "        \n",
    "        Args:\n",
    "            articles: List of preprocessed articles from finbert_preprocessor\n",
    "                Expected format: [{\"id\": \"...\", \"text\": \"...\", \"metadata\": {...}}, ...]\n",
    "        \n",
    "        Returns:\n",
    "            List of sentiment predictions:\n",
    "            [\n",
    "                {\n",
    "                    \"id\": \"unique_id\",\n",
    "                    \"sentiment\": \"positive\" | \"neutral\" | \"negative\",\n",
    "                    \"scores\": {\n",
    "                        \"positive\": 0.82,\n",
    "                        \"neutral\": 0.14,\n",
    "                        \"negative\": 0.04\n",
    "                    }\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        \"\"\"\n",
    "\n",
    "        # ✅ MINIMAL EDIT: load model now (not at startup)\n",
    "        self._lazy_load()\n",
    "\n",
    "        if not articles:\n",
    "            logger.warning(\"No articles provided for sentiment analysis\")\n",
    "            return []\n",
    "        \n",
    "        logger.info(f\"Starting sentiment analysis for {len(articles)} articles/chunks\")\n",
    "        \n",
    "        try:\n",
    "            results = []\n",
    "            \n",
    "            # Process articles in batches\n",
    "            for i in range(0, len(articles), self.batch_size):\n",
    "                batch = articles[i:i + self.batch_size]\n",
    "                batch_results = self._process_batch(batch)\n",
    "                results.extend(batch_results)\n",
    "                \n",
    "                logger.info(f\"Processed batch {i//self.batch_size + 1}/{(len(articles)-1)//self.batch_size + 1}\")\n",
    "            \n",
    "            logger.info(f\"Sentiment analysis complete: {len(results)} predictions generated\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in sentiment analysis: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _process_batch(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a batch of articles through FinBERT.\n",
    "        \n",
    "        Args:\n",
    "            batch: List of article dicts with id, text, metadata\n",
    "        \n",
    "        Returns:\n",
    "            List of sentiment predictions for the batch\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract IDs and texts\n",
    "            ids = [article['id'] for article in batch]\n",
    "            texts = [article['text'] for article in batch]\n",
    "            \n",
    "            # Skip empty texts\n",
    "            valid_indices = [i for i, text in enumerate(texts) if text.strip()]\n",
    "            if not valid_indices:\n",
    "                logger.warning(\"Batch contains only empty texts, skipping\")\n",
    "                return []\n",
    "            \n",
    "            valid_ids = [ids[i] for i in valid_indices]\n",
    "            valid_texts = [texts[i] for i in valid_indices]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            tokenized = self._tokenize_batch(valid_texts)\n",
    "            \n",
    "            # Run inference\n",
    "            logits = self._predict_batch(tokenized)\n",
    "            \n",
    "            # Post-process results\n",
    "            predictions = self._postprocess_logits(logits, valid_ids)\n",
    "            \n",
    "            return predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing batch: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _tokenize_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Tokenize a batch of texts for FinBERT input.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of tokenized tensors ready for model input\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize with padding and truncation\n",
    "            tokenized = self.tokenizer(\n",
    "                texts,\n",
    "                padding=\"longest\",  # Pad to longest sequence in batch\n",
    "                truncation=True,\n",
    "                max_length=512,  # BERT token limit\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move tensors to device\n",
    "            tokenized = {key: val.to(self.device) for key, val in tokenized.items()}\n",
    "            \n",
    "            return tokenized\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error tokenizing batch: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _predict_batch(self, tokenized_batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Run FinBERT forward pass on tokenized batch.\n",
    "        \n",
    "        Args:\n",
    "            tokenized_batch: Dictionary of tokenized tensors\n",
    "        \n",
    "        Returns:\n",
    "            Logits tensor [batch_size, 3]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                outputs = self.model(**tokenized_batch)\n",
    "                logits = outputs.logits  # Shape: [batch_size, 3]\n",
    "            \n",
    "            return logits\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during model inference: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _postprocess_logits(self, logits: torch.Tensor, ids: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Convert logits to sentiment predictions with probabilities.\n",
    "        \n",
    "        Args:\n",
    "            logits: Raw model output tensor [batch_size, 3]\n",
    "            ids: List of article IDs corresponding to each prediction\n",
    "        \n",
    "        Returns:\n",
    "            List of sentiment prediction dicts\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(logits, dim=1)  # Shape: [batch_size, 3]\n",
    "            \n",
    "            # Get predicted class (highest probability)\n",
    "            predicted_classes = torch.argmax(probabilities, dim=1)\n",
    "            \n",
    "            # Convert to CPU and numpy for processing\n",
    "            probabilities = probabilities.cpu().numpy()\n",
    "            predicted_classes = predicted_classes.cpu().numpy()\n",
    "            \n",
    "            # Build result list\n",
    "            predictions = []\n",
    "            for i, article_id in enumerate(ids):\n",
    "                pred_class = int(predicted_classes[i])\n",
    "                sentiment_label = self.label_map[pred_class]\n",
    "                \n",
    "                # Extract probability scores\n",
    "                scores = {\n",
    "                    'positive': float(probabilities[i][0]),\n",
    "                    'neutral': float(probabilities[i][1]),\n",
    "                    'negative': float(probabilities[i][2])\n",
    "                }\n",
    "                \n",
    "                prediction = {\n",
    "                    'id': article_id,\n",
    "                    'sentiment': sentiment_label,\n",
    "                    'scores': scores\n",
    "                }\n",
    "                \n",
    "                predictions.append(prediction)\n",
    "            \n",
    "            return predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error post-processing logits: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "# Test/Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Starting FinBERT client test\")\n",
    "        \n",
    "        # Initialize client (downloads model on first run)\n",
    "        client = FinBERTClient(\n",
    "            model_name=\"yiyanghkust/finbert-tone\",\n",
    "            batch_size=8\n",
    "        )\n",
    "        \n",
    "        # Sample preprocessed articles (matching finbert_preprocessor output format)\n",
    "        sample_articles = [\n",
    "            {\n",
    "                'id': 'a9c73f3c4e',\n",
    "                'text': 'Reliance Industries Reports Strong Q3 Results. Reliance Industries Limited announced robust quarterly results with revenue growth of 15% year-over-year.',\n",
    "                'metadata': {\n",
    "                    'title': 'Reliance Industries Reports Strong Q3 Results',\n",
    "                    'url': 'https://example.com/news/reliance-q3',\n",
    "                    'published': '2025-10-03T09:00:00Z',\n",
    "                    'source': 'gemini',\n",
    "                    'relevance_score': 0.95,\n",
    "                    'weight': 1.0\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'id': 'b2f84a9d1c',\n",
    "                'text': 'Market Update: Tech Stocks Show Weakness. Technology stocks declined today amid concerns about rising interest rates and regulatory scrutiny.',\n",
    "                'metadata': {\n",
    "                    'title': 'Market Update: Tech Stocks Show Weakness',\n",
    "                    'url': 'https://example.com/news/tech-decline',\n",
    "                    'published': '2025-10-03T10:30:00Z',\n",
    "                    'source': 'rss',\n",
    "                    'relevance_score': 0.72,\n",
    "                    'weight': 1.0\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'id': 'c3e95b8a2d',\n",
    "                'text': 'Company announces quarterly dividend. The board approved a dividend payment in line with expectations.',\n",
    "                'metadata': {\n",
    "                    'title': 'Dividend Announcement',\n",
    "                    'url': 'https://example.com/news/dividend',\n",
    "                    'published': '2025-10-03T11:00:00Z',\n",
    "                    'source': 'rss',\n",
    "                    'relevance_score': 0.68,\n",
    "                    'weight': 1.0\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        results = client.analyze(sample_articles)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Sentiment Analysis Results\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Analyzed {len(results)} articles\\n\")\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"ID: {result['id']}\")\n",
    "            print(f\"Sentiment: {result['sentiment'].upper()}\")\n",
    "            print(f\"Scores:\")\n",
    "            print(f\"  - Positive: {result['scores']['positive']:.4f}\")\n",
    "            print(f\"  - Neutral:  {result['scores']['neutral']:.4f}\")\n",
    "            print(f\"  - Negative: {result['scores']['negative']:.4f}\")\n",
    "            print(f\"{'-'*70}\\n\")\n",
    "        \n",
    "        logger.info(\"Test completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in test execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
