{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2514f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from typing import List, Dict, Optional, Any\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from urllib.parse import urlparse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../..\"))\n",
    "dotenv_path = os.path.join(project_root, \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LLMWebSearcher:\n",
    "    \"\"\"\n",
    "    LLM-powered web search for financial news articles.\n",
    "    MVP implementation using Gemini API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gemini\", max_articles: int = 10, date_range_days: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the LLM web searcher.\n",
    "        \n",
    "        Args:\n",
    "            model: LLM provider to use (currently only \"gemini\" supported)\n",
    "            max_articles: Maximum number of articles to return\n",
    "            date_range_days: How many days back to search\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.max_articles = max_articles\n",
    "        self.date_range_days = date_range_days\n",
    "        \n",
    "        # Get API key from environment\n",
    "        self.api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not self.api_key:\n",
    "            logger.warning(\"GEMINI_API_KEY not found in environment variables\")\n",
    "        \n",
    "        # Credible financial sources whitelist\n",
    "        self.credible_sources = [\n",
    "            'economictimes.indiatimes.com',\n",
    "            'business-standard.com',\n",
    "            'livemint.com',\n",
    "            'moneycontrol.com',\n",
    "            'thehindubusinessline.com',\n",
    "            'reuters.com',\n",
    "            'bloomberg.com',\n",
    "            'financialexpress.com',\n",
    "            'zeenews.india.com',\n",
    "            'cnbctv18.com',\n",
    "            'ndtvprofit.com',\n",
    "            'news18.com'\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"LLMWebSearcher initialized: model={model}, max_articles={max_articles}, days={date_range_days}\")\n",
    "    \n",
    "    def build_prompt(self, company_name: str, ticker: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Build a structured prompt for the LLM to return financial news in JSON format.\n",
    "        \n",
    "        Args:\n",
    "            company_name: Company name (e.g., \"Reliance Industries\")\n",
    "            ticker: Optional NSE ticker symbol (e.g., \"RELIANCE\")\n",
    "        \n",
    "        Returns:\n",
    "            Formatted prompt string\n",
    "        \"\"\"\n",
    "        ticker_text = f\" (ticker {ticker})\" if ticker else \"\"\n",
    "        sources_list = \", \".join(self.credible_sources)\n",
    "        \n",
    "        prompt = f\"\"\"You are a financial news search assistant for Indian stock markets. Search the web for the most recent financial news articles from the last {self.date_range_days} days about the company {company_name}{ticker_text}.\n",
    "\n",
    "        Return ONLY a JSON array of objects. Each object must have these exact fields:\n",
    "        - \"title\" (string): Article headline\n",
    "        - \"url\" (string): Full article URL\n",
    "        - \"date\" (string): Publication date in YYYY-MM-DD format\n",
    "        - \"summary\" (string): A detailed 10-sentence factual summary of the article's financial and business content\n",
    "        \n",
    "        SUMMARY REQUIREMENTS:\n",
    "        - Must be around 6 to 7 sentences (full, complete sentences with proper grammar)\n",
    "        - Focus on factual information: financial figures, percentages, dates, announcements, and concrete events\n",
    "        - Include specific numbers whenever available (revenue, profit, EPS, stock price, market cap, etc.)\n",
    "        - Mention NSE/BSE stock performance, trading volumes, and price movements when available\n",
    "        - Include information about quarterly results, annual reports, board decisions, dividend announcements\n",
    "        - Reference key stakeholders: promoters, institutional investors, executives, or analysts quoted\n",
    "        - Mention regulatory filings (SEBI disclosures, BSE/NSE announcements) if applicable\n",
    "        - Avoid vague statements like \"the company is performing well\" - be specific with data\n",
    "        - Do not invent or assume details not present in the article\n",
    "        - Each sentence should add unique, valuable information for investors\n",
    "        \n",
    "        IMPORTANT CONSTRAINTS:\n",
    "        1. Only include articles published within the **last 7 days** from the current date in current month (October,2025). \n",
    "           - Strictly check the published date in the article.\n",
    "           - If the date is missing or ambiguous, include the article.\n",
    "        2. Only include articles from these credible financial sources: {sources_list}\n",
    "        3. Return maximum {self.max_articles} articles\n",
    "        4. Only recent articles (last {self.date_range_days} days)\n",
    "        5. Focus on: quarterly earnings, stock price movements, corporate actions, mergers & acquisitions, management changes, regulatory updates, sectoral trends\n",
    "        6. Do not include any text before or after the JSON array\n",
    "        7. If no articles found, return empty array: []\n",
    "        \n",
    "        The JSON must be well-formed and strictly follow this structure.\n",
    "        \n",
    "        Example output format:\n",
    "        [\n",
    "          {{\n",
    "            \"title\": \"Reliance Industries Reports 12% Jump in Q2 Profit, Declares â‚¹9 Per Share Dividend\",\n",
    "            \"url\": \"https://economictimes.indiatimes.com/markets/stocks/news/reliance-industries-q2-results-profit-dividend/articleshow/123456789.cms\",\n",
    "            \"date\": \"2025-10-01\",\n",
    "            \"summary\": \"Reliance Industries Ltd (NSE: RELIANCE) reported a consolidated net profit of â‚¹19,323 crore for Q2 FY2026, marking a 12% increase compared to â‚¹17,265 crore in the same quarter last year. The company's revenue from operations stood at â‚¹2,35,478 crore, up 8.5% year-on-year, driven by strong performance in the retail and digital services segments. The board of directors declared an interim dividend of â‚¹9 per equity share, with a record date set for October 15, 2025. Reliance Jio added 15.2 million new subscribers during the quarter, taking its total subscriber base to 485 million, while average revenue per user (ARPU) increased to â‚¹195 from â‚¹181 in the previous quarter. The oil-to-chemicals (O2C) business reported an EBITDA of â‚¹13,845 crore, impacted by softer refining margins that averaged $8.2 per barrel. Reliance Retail's revenue grew 17% to â‚¹68,345 crore with the addition of 847 new stores, bringing the total store count to 18,040 across India. The company's net debt reduced to â‚¹1,42,890 crore from â‚¹1,56,234 crore in the previous quarter, reflecting improved cash flow generation. Chairman Mukesh Ambani announced plans to invest â‚¹75,000 crore in clean energy projects over the next three years. On NSE, Reliance shares closed at â‚¹2,847.50, up 4.2% on the day of the announcement, with trading volumes surging to 12.5 million shares. Analysts at ICICI Securities maintained a 'Buy' rating with a revised target price of â‚¹3,200, citing strong growth momentum across all business verticals.\"\n",
    "          }}\n",
    "        ]\"\"\"\n",
    "                \n",
    "        return prompt\n",
    "    \n",
    "    def call_model(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Make API call to Gemini and return raw response.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The structured prompt to send\n",
    "        \n",
    "        Returns:\n",
    "            Raw model response as string\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If API call fails\n",
    "        \"\"\"\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not configured\")\n",
    "        \n",
    "        try:\n",
    "            # Use Gemini 2.5 Flash - the fastest and latest model available\n",
    "            GEMINI_MODEL = \"gemini-2.5-flash\"\n",
    "            # Correct Gemini API endpoint using v1\n",
    "            url = f\"https://generativelanguage.googleapis.com/v1/models/{GEMINI_MODEL}:generateContent?key={self.api_key}\"\n",
    "            \n",
    "            headers = {\n",
    "                'Content-Type': 'application/json'\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"contents\": [{\n",
    "                    \"parts\": [{\n",
    "                        \"text\": prompt\n",
    "                    }]\n",
    "                }],\n",
    "                \"generationConfig\": {\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"topK\": 64,\n",
    "                    \"topP\": 0.95,\n",
    "                    \"maxOutputTokens\": 8000,  # Increased to allow full response\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            logger.debug(f\"Calling Gemini API at: {url.replace(self.api_key, '***API_KEY_HIDDEN***')}\")\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=90)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            logger.debug(\"ðŸ“¡ Full Gemini raw response:\\n%s\", json.dumps(result, indent=2))\n",
    "            \n",
    "            # Extract text from Gemini response structure\n",
    "            if 'candidates' in result and len(result['candidates']) > 0:\n",
    "                candidate = result['candidates'][0]\n",
    "                \n",
    "                # Check for truncated response\n",
    "                finish_reason = candidate.get('finishReason', '')\n",
    "                if finish_reason == 'MAX_TOKENS':\n",
    "                    logger.warning(\"âš ï¸ Response was truncated due to MAX_TOKENS. Increase maxOutputTokens.\")\n",
    "                \n",
    "                logger.debug(f\"Candidate keys: {candidate.keys()}\")\n",
    "                \n",
    "                content = candidate.get('content', {})\n",
    "                logger.debug(f\"Content type: {type(content)}, Content keys: {content.keys() if isinstance(content, dict) else 'not a dict'}\")\n",
    "                \n",
    "                parts = content.get('parts', [])\n",
    "                logger.debug(f\"Parts: {parts}\")\n",
    "                \n",
    "                if parts and len(parts) > 0:\n",
    "                    logger.debug(f\"First part keys: {parts[0].keys() if isinstance(parts[0], dict) else 'not a dict'}\")\n",
    "                    if 'text' in parts[0]:\n",
    "                        text = parts[0]['text']\n",
    "                        logger.debug(f\"âœ… Received response text ({len(text)} chars)\")\n",
    "                        return text\n",
    "                    else:\n",
    "                        logger.warning(f\"No 'text' key in first part. Part content: {parts[0]}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No parts in content. Finish reason: {finish_reason}\")\n",
    "            \n",
    "            logger.warning(\"âš ï¸ Unexpected Gemini response structure\")\n",
    "            logger.warning(f\"Response keys: {result.keys()}\")\n",
    "            if 'candidates' in result and result['candidates']:\n",
    "                logger.warning(f\"First candidate: {json.dumps(result['candidates'][0], indent=2)}\")\n",
    "            return \"\"\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.error(\"Gemini API call timed out\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # FIXED: Mask the API key in error logs\n",
    "            error_msg = str(e)\n",
    "            if self.api_key and self.api_key in error_msg:\n",
    "                error_msg = error_msg.replace(self.api_key, \"***API_KEY_HIDDEN***\")\n",
    "            logger.error(f\"Gemini API request failed: {error_msg}\")\n",
    "            raise Exception(error_msg)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calling Gemini: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def parse_response(self, response: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Parse and normalize JSON response from LLM into standardized article format.\n",
    "        \n",
    "        Args:\n",
    "            response: Raw response string from LLM\n",
    "        \n",
    "        Returns:\n",
    "            List of normalized article dictionaries\n",
    "        \"\"\"\n",
    "        if not response or not response.strip():\n",
    "            logger.warning(\"Empty response from LLM\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Clean response - remove markdown code blocks if present\n",
    "            cleaned = response.strip()\n",
    "            if cleaned.startswith('```json'):\n",
    "                cleaned = cleaned[7:]\n",
    "            if cleaned.startswith('```'):\n",
    "                cleaned = cleaned[3:]\n",
    "            if cleaned.endswith('```'):\n",
    "                cleaned = cleaned[:-3]\n",
    "            cleaned = cleaned.strip().lstrip('json').strip()\n",
    "            \n",
    "            logger.debug(f\"Cleaned response to parse: {cleaned[:500]}...\")\n",
    "            \n",
    "            # Parse JSON\n",
    "            raw_articles = json.loads(cleaned)\n",
    "            \n",
    "            if not isinstance(raw_articles, list):\n",
    "                logger.warning(\"Response is not a JSON array\")\n",
    "                return []\n",
    "            \n",
    "            logger.info(f\"Parsed {len(raw_articles)} raw articles from LLM\")\n",
    "            \n",
    "            # Normalize each article\n",
    "            normalized_articles = []\n",
    "            for raw in raw_articles:\n",
    "                try:\n",
    "                    normalized = self._normalize_article(raw)\n",
    "                    if normalized:\n",
    "                        normalized_articles.append(normalized)\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Failed to normalize article: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"Successfully normalized {len(normalized_articles)} articles\")\n",
    "            return normalized_articles\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Failed to parse JSON response: {str(e)}\")\n",
    "            logger.error(f\"Raw response (first 1000 chars):\\n{response[:1000]}\")\n",
    "            logger.error(f\"Cleaned response (first 1000 chars):\\n{cleaned[:1000] if 'cleaned' in locals() else 'N/A'}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing response: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _normalize_article(self, raw_article: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Normalize a single article to match pipeline standard format.\n",
    "        \n",
    "        Args:\n",
    "            raw_article: Raw article dict from LLM\n",
    "        \n",
    "        Returns:\n",
    "            Normalized article dict or None if invalid\n",
    "        \"\"\"\n",
    "        # Extract required fields\n",
    "        title = raw_article.get('title', '').strip()\n",
    "        url = raw_article.get('url', '').strip()\n",
    "        summary = raw_article.get('summary', '').strip()\n",
    "        date_str = raw_article.get('date', '').strip()\n",
    "        \n",
    "        # Validate required fields\n",
    "        if not title or not url:\n",
    "            logger.debug(\"Article missing title or url\")\n",
    "            return None\n",
    "        \n",
    "        # Validate URL\n",
    "        if not self._is_valid_url(url):\n",
    "            logger.debug(f\"Invalid URL: {url}\")\n",
    "            return None\n",
    "        \n",
    "        # Check if URL is from credible source\n",
    "        if not self._is_credible_source(url):\n",
    "            logger.debug(f\"URL not from credible source: {url}\")\n",
    "            return None\n",
    "        \n",
    "        # Parse date\n",
    "        published = self._parse_date(date_str)\n",
    "        \n",
    "        # Create normalized article matching pipeline standard format\n",
    "        normalized = {\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"published\": published,\n",
    "            \"summary\": summary,\n",
    "            \"source\": \"gemini\",\n",
    "            \"relevance_score\": 1.0,\n",
    "            \"weight\": 1.0\n",
    "        }\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def _is_valid_url(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL is valid.\"\"\"\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def _is_credible_source(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL is from a credible source.\"\"\"\n",
    "        try:\n",
    "            domain = urlparse(url).netloc.lower()\n",
    "            # Remove www. prefix\n",
    "            domain = domain.replace('www.', '')\n",
    "            \n",
    "            # Exact match or proper subdomain match\n",
    "            return any(domain == src or domain.endswith(f\".{src}\") for src in self.credible_sources)\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def _parse_date(self, date_str: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Parse date string to datetime object.\n",
    "        \n",
    "        Args:\n",
    "            date_str: Date string (expected format: YYYY-MM-DD)\n",
    "        \n",
    "        Returns:\n",
    "            datetime object (defaults to now if parsing fails)\n",
    "        \"\"\"\n",
    "        if not date_str:\n",
    "            return datetime.now(timezone.utc)\n",
    "        \n",
    "        try:\n",
    "            # Try YYYY-MM-DD format\n",
    "            dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            return dt.replace(tzinfo=timezone.utc)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # Try ISO format\n",
    "                return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "            except Exception:\n",
    "                logger.debug(f\"Could not parse date: {date_str}\")\n",
    "                return datetime.now(timezone.utc)\n",
    "    \n",
    "    def search_news(self, company_name: str, ticker: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Main entry point: Search for financial news about a company.\n",
    "        \n",
    "        Args:\n",
    "            company_name: Company name (e.g., \"Reliance Industries\")\n",
    "            ticker: Optional NSE ticker symbol (e.g., \"RELIANCE\")\n",
    "        \n",
    "        Returns:\n",
    "            List of normalized article dictionaries (empty list on failure)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Searching news for: {company_name} ({ticker or 'no ticker'})\")\n",
    "        \n",
    "        try:\n",
    "            # Build prompt\n",
    "            prompt = self.build_prompt(company_name, ticker)\n",
    "            \n",
    "            # Call model\n",
    "            response = self.call_model(prompt)\n",
    "            \n",
    "            # Parse and normalize\n",
    "            articles = self.parse_response(response)\n",
    "            \n",
    "            logger.info(f\"Search completed: {len(articles)} articles found\")\n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching news: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "# Factory function\n",
    "def create_llm_searcher(model: str = \"gemini\", max_articles: int = 20, date_range_days: int = 3) -> LLMWebSearcher:\n",
    "    \"\"\"Factory function to create LLMWebSearcher instance.\"\"\"\n",
    "    return LLMWebSearcher(model, max_articles, date_range_days)\n",
    "\n",
    "\n",
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize searcher\n",
    "        searcher = LLMWebSearcher()\n",
    "        \n",
    "        print(\"=== LLM Web Searcher Test ===\\n\")\n",
    "        \n",
    "        # Test search\n",
    "        print(\"Searching for: Reliance Industries (RELIANCE)\")\n",
    "        articles = searcher.search_news(\"Reliance Industries\", \"RELIANCE\")\n",
    "        \n",
    "        if articles:\n",
    "            print(f\"\\nFound {len(articles)} articles:\\n\")\n",
    "            for i, article in enumerate(articles[:3], 1):\n",
    "                print(f\"{i}. {article['title'][:70]}...\")\n",
    "                print(f\"   Source: {article['source']}\")\n",
    "                print(f\"   URL: {article['url']}\")\n",
    "                print(f\"   Published: {article['published']}\")\n",
    "                print(f\"   Summary: {article['summary'][:100]}...\\n\")\n",
    "        else:\n",
    "            print(\"No articles found or error occurred\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
