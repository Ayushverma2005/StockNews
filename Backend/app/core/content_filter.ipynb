{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3971a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Any, Set\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import hashlib\n",
    "import unicodedata\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ContentFilter:\n",
    "    \"\"\"Simplified content filter focused on deduplication and basic cleanup\"\"\"\n",
    "    \n",
    "    def __init__(self, relevance_threshold=0.05, max_articles_per_company=50, max_days_old=7):\n",
    "        \"\"\"Initialize with very permissive threshold for now\"\"\"\n",
    "        self.relevance_threshold = relevance_threshold  # Much lower threshold\n",
    "        self.max_articles_per_company = max_articles_per_company\n",
    "        self.max_days_old = max_days_old\n",
    "        \n",
    "        # Simplified stop words for cleanup\n",
    "        self.stop_words = self._load_stop_words()\n",
    "        \n",
    "        logger.info(f\"ContentFilter initialized: threshold={relevance_threshold}, max_articles={max_articles_per_company}\")\n",
    "    \n",
    "    def _load_stop_words(self) -> Set[str]:\n",
    "        \"\"\"Load common stop words to ignore during matching\"\"\"\n",
    "        return {\n",
    "            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be',\n",
    "            'have', 'has', 'had', 'will', 'would', 'could', 'should',\n",
    "            'a', 'an', 'this', 'that', 'these', 'those', 'ltd', 'limited',\n",
    "            'pvt', 'private', 'company', 'corp', 'corporation', 'inc'\n",
    "        }\n",
    "    \n",
    "    def filter_company_articles(self, articles: List[Dict[str, Any]], company_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Simplified filtering: Basic relevance check + deduplication + sorting\n",
    "        Maintains same input/output format for pipeline compatibility\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not articles or not company_data:\n",
    "                logger.warning(\"Empty articles or company_data provided\")\n",
    "                return []\n",
    "            \n",
    "            logger.info(f\"Processing {len(articles)} articles for {company_data.get('company_name', 'Unknown')}\")\n",
    "            \n",
    "            # Step 1: Basic relevance filtering (very permissive)\n",
    "            search_terms = [t.lower() for t in company_data.get('search_terms', []) if isinstance(t, str)]\n",
    "            relevant_articles = []\n",
    "            for article in articles:\n",
    "                relevance_score = self.check_article_relevance(\n",
    "                    article, \n",
    "                    company_data.get('company_name', ''), \n",
    "                    company_data.get('symbol', '')\n",
    "                )\n",
    "                # Boost if any search term appears in title/description (substring match)\n",
    "                try:\n",
    "                    text = f\"{article.get('title', '')} {article.get('description', '')}\".lower()\n",
    "                    if search_terms and any(term in text for term in search_terms):\n",
    "                        relevance_score = min(1.0, relevance_score + 0.25)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                \n",
    "                if relevance_score >= self.relevance_threshold:\n",
    "                    article['relevance_score'] = relevance_score\n",
    "                    # Add company identifiers for compatibility\n",
    "                    article['company_identifiers_found'] = self._find_basic_matches(article, company_data)\n",
    "                    relevant_articles.append(article)\n",
    "            \n",
    "            logger.info(f\"Found {len(relevant_articles)} relevant articles (threshold: {self.relevance_threshold})\")\n",
    "            \n",
    "            if not relevant_articles:\n",
    "                return []\n",
    "            \n",
    "            # Step 2: Remove duplicates\n",
    "            deduplicated_articles = self.deduplicate_articles(relevant_articles)\n",
    "            \n",
    "            # Step 3: Filter by date\n",
    "            date_filtered_articles = self._filter_by_date(deduplicated_articles)\n",
    "            \n",
    "            # Step 4: Sort by relevance and recency\n",
    "            sorted_articles = self.sort_by_recency(date_filtered_articles)\n",
    "            \n",
    "            # Step 5: Limit number of articles\n",
    "            final_articles = sorted_articles[:self.max_articles_per_company]\n",
    "            \n",
    "            logger.info(f\"Final result: {len(final_articles)} articles for {company_data.get('symbol', 'Unknown')}\")\n",
    "            \n",
    "            return final_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in filter_company_articles: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def check_article_relevance(self, article: Dict[str, Any], company_name: str, ticker: str) -> float:\n",
    "        \"\"\"\n",
    "        Simplified relevance check - very permissive, focuses on basic company matching\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not article:\n",
    "                return 0.0\n",
    "            \n",
    "            # Get article text content\n",
    "            title = self._clean_text(article.get('title', ''))\n",
    "            description = self._clean_text(article.get('description', ''))\n",
    "            article_text = f\"{title} {description}\".lower()\n",
    "            \n",
    "            if not article_text.strip():\n",
    "                return 0.0\n",
    "            \n",
    "            relevance_score = 0.0\n",
    "            \n",
    "            # 1. Direct ticker matching (if available)\n",
    "            if ticker and ticker.lower() in article_text:\n",
    "                relevance_score += 0.8  # High confidence\n",
    "                logger.debug(f\"Direct ticker match: {ticker}\")\n",
    "            \n",
    "            # 2. Company name matching (various forms)\n",
    "            if company_name:\n",
    "                name_score = self._simple_company_match(article_text, company_name)\n",
    "                relevance_score += name_score * 0.6\n",
    "            \n",
    "            # 3. Basic business context (very broad)\n",
    "            if self._has_business_context(article_text):\n",
    "                relevance_score += 0.1\n",
    "            \n",
    "            # 4. Not obviously irrelevant (weather, sports, etc.)\n",
    "            if not self._is_obviously_irrelevant(article_text):\n",
    "                relevance_score += 0.05\n",
    "            \n",
    "            # Ensure minimum score for any article with company mention\n",
    "            if ticker and ticker.lower() in article_text:\n",
    "                relevance_score = max(relevance_score, 0.1)\n",
    "            elif company_name and company_name.lower() in article_text:\n",
    "                relevance_score = max(relevance_score, 0.08)\n",
    "            \n",
    "            return min(relevance_score, 1.0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating relevance: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _simple_company_match(self, article_text: str, company_name: str) -> float:\n",
    "        \"\"\"Simplified company name matching\"\"\"\n",
    "        if not company_name:\n",
    "            return 0.0\n",
    "        \n",
    "        company_lower = company_name.lower()\n",
    "        \n",
    "        # Exact match\n",
    "        if company_lower in article_text:\n",
    "            return 1.0\n",
    "        \n",
    "        # Clean version without suffixes\n",
    "        clean_name = self._clean_company_name(company_name).lower()\n",
    "        if clean_name and clean_name in article_text:\n",
    "            return 0.8\n",
    "        \n",
    "        # First word match (for common abbreviations)\n",
    "        first_word = company_lower.split()[0] if company_lower.split() else \"\"\n",
    "        if len(first_word) > 3 and first_word in article_text:\n",
    "            return 0.3\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def _clean_company_name(self, company_name: str) -> str:\n",
    "        \"\"\"Remove common company suffixes\"\"\"\n",
    "        if not company_name:\n",
    "            return \"\"\n",
    "        \n",
    "        clean_name = company_name.strip()\n",
    "        suffixes = [' Ltd', ' Limited', ' Pvt', ' Private', ' Company', ' Corp', ' Corporation', ' Inc']\n",
    "        \n",
    "        for suffix in suffixes:\n",
    "            if clean_name.endswith(suffix):\n",
    "                clean_name = clean_name[:-len(suffix)].strip()\n",
    "                break\n",
    "        \n",
    "        return clean_name\n",
    "    \n",
    "    def _has_business_context(self, article_text: str) -> bool:\n",
    "        \"\"\"Check for basic business/financial context\"\"\"\n",
    "        business_terms = [\n",
    "            'revenue', 'profit', 'loss', 'earnings', 'shares', 'stock', 'market',\n",
    "            'business', 'company', 'financial', 'investment', 'growth', 'results'\n",
    "        ]\n",
    "        return any(term in article_text for term in business_terms)\n",
    "    \n",
    "    def _is_obviously_irrelevant(self, article_text: str) -> bool:\n",
    "        \"\"\"Check for obviously irrelevant content\"\"\"\n",
    "        irrelevant_terms = [\n",
    "            'weather', 'rainfall', 'temperature', 'sports', 'cricket', 'football',\n",
    "            'movie', 'film', 'celebrity', 'entertainment', 'recipe', 'cooking'\n",
    "        ]\n",
    "        return any(term in article_text for term in irrelevant_terms)\n",
    "    \n",
    "    def _find_basic_matches(self, article: Dict[str, Any], company_data: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Find basic company matches for compatibility with existing pipeline\"\"\"\n",
    "        article_text = f\"{article.get('title', '')} {article.get('description', '')}\".lower()\n",
    "        found_matches = []\n",
    "        \n",
    "        # Check symbol\n",
    "        symbol = company_data.get('symbol', '')\n",
    "        if symbol and symbol.lower() in article_text:\n",
    "            found_matches.append(f\"symbol:{symbol}\")\n",
    "        \n",
    "        # Check company name\n",
    "        company_name = company_data.get('company_name', '')\n",
    "        if company_name and company_name.lower() in article_text:\n",
    "            found_matches.append(f\"company:{company_name}\")\n",
    "        \n",
    "        return found_matches\n",
    "    \n",
    "    def deduplicate_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Remove duplicate articles - keeping the deduplication logic as it's solid\"\"\"\n",
    "        try:\n",
    "            if not articles:\n",
    "                return []\n",
    "            \n",
    "            logger.debug(f\"Deduplicating {len(articles)} articles\")\n",
    "            \n",
    "            seen_hashes = set()\n",
    "            seen_urls = set()\n",
    "            seen_titles = {}\n",
    "            unique_articles = []\n",
    "            \n",
    "            for article in articles:\n",
    "                # Generate content hash\n",
    "                content_hash = self._generate_article_hash(article)\n",
    "                url = self._clean_url(article.get('link', ''))\n",
    "                title = self._clean_text(article.get('title', ''))\n",
    "                title_normalized = self._normalize_title(title)\n",
    "                \n",
    "                is_duplicate = False\n",
    "                \n",
    "                # Check for duplicates\n",
    "                if content_hash and content_hash in seen_hashes:\n",
    "                    is_duplicate = True\n",
    "                elif url and url in seen_urls:\n",
    "                    is_duplicate = True\n",
    "                elif title_normalized:\n",
    "                    for existing_title_norm, existing_article in seen_titles.items():\n",
    "                        similarity = self._calculate_title_similarity(title_normalized, existing_title_norm)\n",
    "                        if similarity > 0.85:\n",
    "                            is_duplicate = True\n",
    "                            # Keep the one with higher relevance score\n",
    "                            if article.get('relevance_score', 0) > existing_article.get('relevance_score', 0):\n",
    "                                unique_articles.remove(existing_article)\n",
    "                                seen_titles[title_normalized] = article\n",
    "                                unique_articles.append(article)\n",
    "                            break\n",
    "                \n",
    "                if not is_duplicate:\n",
    "                    if content_hash:\n",
    "                        seen_hashes.add(content_hash)\n",
    "                    if url:\n",
    "                        seen_urls.add(url)\n",
    "                    if title_normalized:\n",
    "                        seen_titles[title_normalized] = article\n",
    "                    unique_articles.append(article)\n",
    "            \n",
    "            logger.info(f\"Deduplication: {len(articles)} -> {len(unique_articles)} articles\")\n",
    "            return unique_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in deduplicate_articles: {str(e)}\")\n",
    "            return articles\n",
    "    \n",
    "    def _generate_article_hash(self, article: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate hash for article content comparison\"\"\"\n",
    "        try:\n",
    "            title = self._clean_text(article.get('title', ''))\n",
    "            description = self._clean_text(article.get('description', ''))[:200]\n",
    "            content = f\"{title}|{description}\".lower()\n",
    "            return hashlib.md5(content.encode()).hexdigest() if content.strip() else \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    \n",
    "    def _clean_url(self, url: str) -> str:\n",
    "        \"\"\"Clean and normalize URL for comparison\"\"\"\n",
    "        if not url:\n",
    "            return \"\"\n",
    "        url = re.sub(r'[?&](utm_|ref=|source=|campaign=)[^&]*', '', url)\n",
    "        url = re.sub(r'[/#]+$', '', url)\n",
    "        return url.lower().strip()\n",
    "    \n",
    "    def _normalize_title(self, title: str) -> str:\n",
    "        \"\"\"Normalize title for similarity comparison\"\"\"\n",
    "        if not title:\n",
    "            return \"\"\n",
    "        normalized = re.sub(r'\\s+', ' ', title.lower().strip())\n",
    "        normalized = re.sub(r'[^\\w\\s]', ' ', normalized)\n",
    "        words = normalized.split()\n",
    "        significant_words = [w for w in words if len(w) > 2 and w not in self.stop_words]\n",
    "        return ' '.join(significant_words)\n",
    "    \n",
    "    def _calculate_title_similarity(self, title1: str, title2: str) -> float:\n",
    "        \"\"\"Calculate Jaccard similarity between titles\"\"\"\n",
    "        if not title1 or not title2:\n",
    "            return 0.0\n",
    "        \n",
    "        words1 = set(title1.split())\n",
    "        words2 = set(title2.split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def sort_by_recency(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Sort by relevance score then recency\"\"\"\n",
    "        try:\n",
    "            if not articles:\n",
    "                return []\n",
    "            \n",
    "            logger.debug(f\"Sorting {len(articles)} articles\")\n",
    "            \n",
    "            for article in articles:\n",
    "                parsed_date = self._parse_article_date(article.get('published', ''))\n",
    "                article['_parsed_date'] = parsed_date\n",
    "                article['_relevance_score'] = article.get('relevance_score', 0.0)\n",
    "            \n",
    "            sorted_articles = sorted(\n",
    "                articles,\n",
    "                key=lambda x: (x['_relevance_score'], x['_parsed_date']),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Clean up temporary fields\n",
    "            for article in sorted_articles:\n",
    "                article.pop('_parsed_date', None)\n",
    "                article.pop('_relevance_score', None)\n",
    "            \n",
    "            return sorted_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error sorting articles: {str(e)}\")\n",
    "            return articles\n",
    "    \n",
    "    def _parse_article_date(self, date_str: str) -> datetime:\n",
    "        \"\"\"Parse publication date - simplified version\"\"\"\n",
    "        if not date_str:\n",
    "            return datetime.min.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        try:\n",
    "            # Handle ISO format with Z\n",
    "            if 'T' in date_str and 'Z' in date_str:\n",
    "                return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "            \n",
    "            # Handle ISO format\n",
    "            if 'T' in date_str:\n",
    "                return datetime.fromisoformat(date_str)\n",
    "            \n",
    "            # Try common formats\n",
    "            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d']:\n",
    "                try:\n",
    "                    return datetime.strptime(date_str, fmt).replace(tzinfo=timezone.utc)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # Default to now if parsing fails\n",
    "            return datetime.now(timezone.utc)\n",
    "            \n",
    "        except Exception:\n",
    "            return datetime.now(timezone.utc)\n",
    "    \n",
    "    def _filter_by_date(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Filter articles by date\"\"\"\n",
    "        try:\n",
    "            if not articles:\n",
    "                return []\n",
    "            \n",
    "            cutoff_date = datetime.now(timezone.utc) - timedelta(days=self.max_days_old)\n",
    "            filtered_articles = []\n",
    "            \n",
    "            for article in articles:\n",
    "                article_date = self._parse_article_date(article.get('published', ''))\n",
    "                if article_date >= cutoff_date:\n",
    "                    filtered_articles.append(article)\n",
    "            \n",
    "            logger.info(f\"Date filtering: {len(articles)} -> {len(filtered_articles)} articles\")\n",
    "            return filtered_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error filtering by date: {str(e)}\")\n",
    "            return articles\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text content\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Normalize unicode\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_filter_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get filter statistics for compatibility\"\"\"\n",
    "        return {\n",
    "            'configuration': {\n",
    "                'relevance_threshold': self.relevance_threshold,\n",
    "                'max_articles_per_company': self.max_articles_per_company,\n",
    "                'max_days_old': self.max_days_old\n",
    "            },\n",
    "            'keyword_categories': {'simplified': 1},  # Placeholder for compatibility\n",
    "            'total_keywords': 0,  # No longer using keyword lists\n",
    "            'stop_words_count': len(self.stop_words)\n",
    "        }\n",
    "\n",
    "# Keep original factory function for compatibility\n",
    "def create_content_filter(relevance_threshold=0.05, max_articles=50, max_days=30) -> ContentFilter: #change to 7\n",
    "    \"\"\"Factory function to create ContentFilter instance\"\"\"\n",
    "    return ContentFilter(relevance_threshold, max_articles, max_days)\n",
    "\n",
    "# Simplified test section\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize with very permissive settings\n",
    "        content_filter = ContentFilter(relevance_threshold=0.05)\n",
    "        \n",
    "        print(\"=== Simplified Content Filter Test ===\")\n",
    "        \n",
    "        # Test with recent dates\n",
    "        from datetime import datetime, timezone\n",
    "        recent_date = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        \n",
    "        sample_articles = [\n",
    "            {\n",
    "                'title': 'TCS Q3 Results: Revenue grows 12% YoY, profit jumps 15%',\n",
    "                'description': 'Tata Consultancy Services reported strong quarterly results.',\n",
    "                'link': 'https://example.com/tcs-results-1',\n",
    "                'published': recent_date,  # Use current date\n",
    "                'source': 'Economic Times'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Weather update: Heavy rainfall expected in Mumbai',\n",
    "                'description': 'Meteorological department warning for Mumbai.',\n",
    "                'link': 'https://example.com/weather-news',\n",
    "                'published': recent_date,  # Use current date\n",
    "                'source': 'Moneycontrol'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        company_data = {\n",
    "            'symbol': 'TCS',\n",
    "            'company_name': 'Tata Consultancy Services Limited'\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTesting with {len(sample_articles)} articles\")\n",
    "        \n",
    "        # Test filtering\n",
    "        filtered_articles = content_filter.filter_company_articles(sample_articles, company_data)\n",
    "        print(f\"Filtered articles: {len(filtered_articles)}\")\n",
    "        \n",
    "        for i, article in enumerate(filtered_articles, 1):\n",
    "            print(f\"{i}. {article['title']}\")\n",
    "            print(f\"   Score: {article.get('relevance_score', 0):.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Test error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
