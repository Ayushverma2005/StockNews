{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation.py\n",
    "\"\"\"\n",
    "Evaluate fine-tuned model on gold test set and compare with baseline.\n",
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(__file__).parent))\n",
    "from utils import LABEL2ID, ID2LABEL\n",
    "from config import DATA_DIR, OUTPUT_DIR, MAX_LENGTH\n",
    "\n",
    "\n",
    "def evaluate_finetuned_model(test_csv_path: str, model_dir: str):\n",
    "    \"\"\"\n",
    "    Evaluate fine-tuned model on manually labeled test set.\n",
    "    \n",
    "    Args:\n",
    "        test_csv_path: Path to CSV with text and manual_label columns\n",
    "        model_dir: Path to fine-tuned model directory\n",
    "    \"\"\"\n",
    "    # Check if files exist\n",
    "    test_path = Path(test_csv_path)\n",
    "    model_path = Path(model_dir)\n",
    "    \n",
    "    if not test_path.exists():\n",
    "        raise FileNotFoundError(f\"Test file not found: {test_csv_path}\")\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Fine-tuned Model Evaluation\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Test file: {test_csv_path}\")\n",
    "    print(f\"Model: {model_dir}\\n\")\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test data...\")\n",
    "    df = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    if 'text' not in df.columns or 'manual_label' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'text' and 'manual_label' columns\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df)} test samples\")\n",
    "    \n",
    "    # Load fine-tuned model and tokenizer\n",
    "    print(\"\\nLoading fine-tuned model...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded on {device}\\n\")\n",
    "    \n",
    "    # Run predictions\n",
    "    print(\"Running predictions...\")\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in df['text']:\n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Predict\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            pred_id = torch.argmax(logits, dim=1).item()\n",
    "            pred_label = ID2LABEL[pred_id]\n",
    "            \n",
    "            predictions.append(pred_label)\n",
    "    \n",
    "    df['finetuned_label'] = predictions\n",
    "    \n",
    "    print(f\"‚úÖ Generated predictions for {len(df)} samples\")\n",
    "    print(f\"\\nFine-tuned label distribution:\")\n",
    "    print(df['finetuned_label'].value_counts())\n",
    "    \n",
    "    # Save results\n",
    "    output_path = test_path.parent / \"real_test_finetuned_predictions.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Saved predictions to: {output_path}\")\n",
    "    \n",
    "    # Compute metrics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION METRICS - Fine-tuned Model\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"-\"*70)\n",
    "    print(classification_report(\n",
    "        df['manual_label'],\n",
    "        df['finetuned_label'],\n",
    "        target_names=['positive', 'neutral', 'negative'],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"              Predicted\")\n",
    "    print(\"              positive  neutral  negative\")\n",
    "    cm = confusion_matrix(\n",
    "        df['manual_label'],\n",
    "        df['finetuned_label'],\n",
    "        labels=['positive', 'neutral', 'negative']\n",
    "    )\n",
    "    for i, label in enumerate(['positive', 'neutral', 'negative']):\n",
    "        print(f\"Actual {label:8s}  {cm[i][0]:8d}  {cm[i][1]:7d}  {cm[i][2]:8d}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "def compare_with_baseline(test_csv_path: str):\n",
    "    \"\"\"\n",
    "    Compare fine-tuned model with baseline FinBERT.\n",
    "    \n",
    "    Args:\n",
    "        test_csv_path: Path to CSV with manual_label, finbert_label, finetuned_label\n",
    "    \"\"\"\n",
    "    test_path = Path(test_csv_path)\n",
    "    \n",
    "    if not test_path.exists():\n",
    "        raise FileNotFoundError(f\"Test file not found: {test_csv_path}\")\n",
    "    \n",
    "    df = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    # Check if both baseline and fine-tuned predictions exist\n",
    "    if 'finbert_label' not in df.columns or 'finetuned_label' not in df.columns:\n",
    "        print(\"‚ö†Ô∏è  Missing baseline or fine-tuned predictions for comparison\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BASELINE vs FINE-TUNED COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    \n",
    "    # Baseline metrics\n",
    "    baseline_acc = accuracy_score(df['manual_label'], df['finbert_label'])\n",
    "    baseline_f1 = f1_score(df['manual_label'], df['finbert_label'], average='weighted', zero_division=0)\n",
    "    \n",
    "    # Fine-tuned metrics\n",
    "    finetuned_acc = accuracy_score(df['manual_label'], df['finetuned_label'])\n",
    "    finetuned_f1 = f1_score(df['manual_label'], df['finetuned_label'], average='weighted', zero_division=0)\n",
    "    \n",
    "    print(\"\\nüìä Performance Summary:\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Metric':<20} {'Baseline':<15} {'Fine-tuned':<15} {'Improvement':<15}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Accuracy':<20} {baseline_acc:<15.4f} {finetuned_acc:<15.4f} {finetuned_acc-baseline_acc:+.4f}\")\n",
    "    print(f\"{'F1 Score (weighted)':<20} {baseline_f1:<15.4f} {finetuned_f1:<15.4f} {finetuned_f1-baseline_f1:+.4f}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    if finetuned_acc > baseline_acc:\n",
    "        print(f\"\\n‚úÖ Fine-tuning improved accuracy by {(finetuned_acc-baseline_acc)*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Fine-tuning decreased accuracy by {(baseline_acc-finetuned_acc)*100:.2f}%\")\n",
    "    \n",
    "        # ---- SAVE RESULTS AS JSON ----\n",
    "    \n",
    "\n",
    "    # Get structured classification report\n",
    "    report_dict = classification_report(\n",
    "        df['manual_label'],\n",
    "        df['finetuned_label'],\n",
    "        target_names=['positive', 'neutral', 'negative'],\n",
    "        zero_division=0,\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    # Convert confusion matrix to list for JSON serialization\n",
    "    cm_list = cm.tolist()\n",
    "\n",
    "    json_output = {\n",
    "        \"model\": \"finetuned\",\n",
    "        \"test_samples\": len(df),\n",
    "        \"classification_report\": report_dict,\n",
    "        \"confusion_matrix\": cm_list\n",
    "    }\n",
    "\n",
    "    json_path = test_path.parent / \"evaluation_report.json\"\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(json_output, f, indent=4)\n",
    "\n",
    "    print(f\"\\n‚úÖ Saved JSON report to: {json_path}\")\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    comparison_output = {\n",
    "        \"baseline\": {\n",
    "            \"accuracy\": baseline_acc,\n",
    "            \"f1_score\": baseline_f1\n",
    "        },\n",
    "        \"finetuned\": {\n",
    "            \"accuracy\": finetuned_acc,\n",
    "            \"f1_score\": finetuned_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    json_path = test_path.parent / \"comparison_summary.json\"\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(comparison_output, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Saved comparison JSON to: {json_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"CLI entrypoint for evaluation.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Evaluate fine-tuned model on gold test set\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_csv\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to test CSV file (default: DATA_DIR/real_test.csv)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_dir\",\n",
    "        type=str,\n",
    "        default=OUTPUT_DIR,\n",
    "        help=f\"Fine-tuned model directory (default: {OUTPUT_DIR})\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--compare\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Compare with baseline (requires real_test_with_predictions.csv)\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Use default path if not specified\n",
    "    if args.test_csv is None:\n",
    "        test_csv_path = Path(DATA_DIR) / \"real_test.csv\"\n",
    "    else:\n",
    "        test_csv_path = Path(args.test_csv)\n",
    "    \n",
    "    try:\n",
    "        # Evaluate fine-tuned model\n",
    "        evaluate_finetuned_model(str(test_csv_path), args.model_dir)\n",
    "        \n",
    "        # Compare with baseline if requested\n",
    "        if args.compare:\n",
    "            comparison_csv = Path(DATA_DIR) / \"real_test_finetuned_predictions.csv\"\n",
    "            compare_with_baseline(str(comparison_csv))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Evaluation failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
