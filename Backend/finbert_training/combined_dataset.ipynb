{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d39fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_dataset.py\n",
    "\"\"\"\n",
    "Merge synthetic and real (pseudo-labeled) datasets into combined train/val/test splits.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "from app.core.finbert_client import FinBERTClient\n",
    "\n",
    "\n",
    "def load_synthetic(data_dir: str = \"./data\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load synthetic train and val datasets.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing synthetic CSV files\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with text and label columns\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If synthetic files not found\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    train_path = data_path / \"synthetic_train.csv\"\n",
    "    val_path = data_path / \"synthetic_val.csv\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not train_path.exists():\n",
    "        raise FileNotFoundError(f\"Synthetic train file not found: {train_path}\")\n",
    "    if not val_path.exists():\n",
    "        raise FileNotFoundError(f\"Synthetic val file not found: {val_path}\")\n",
    "    \n",
    "    print(f\"Loading synthetic datasets from {data_dir}...\")\n",
    "    \n",
    "    # Load both files\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    \n",
    "    # Concatenate\n",
    "    synth_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"✅ Loaded {len(synth_df)} synthetic samples\")\n",
    "    print(f\"   Train: {len(train_df)}, Val: {len(val_df)}\")\n",
    "    print(f\"   Class distribution:\\n{synth_df['label'].value_counts()}\")\n",
    "    \n",
    "    return synth_df\n",
    "\n",
    "\n",
    "def load_real(data_dir: str = \"./data\", auto_label: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load real news export and optionally apply pseudo-labels using FinBERT.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing news_export.csv\n",
    "        auto_label: If True, run FinBERT to assign pseudo-labels\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with text and label columns\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If news export file not found\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    news_path = data_path / \"news_export.csv\"\n",
    "    \n",
    "    if not news_path.exists():\n",
    "        raise FileNotFoundError(f\"News export file not found: {news_path}\")\n",
    "    \n",
    "    print(f\"\\nLoading real news from {news_path}...\")\n",
    "    \n",
    "    # Load news export\n",
    "    real_df = pd.read_csv(news_path)\n",
    "    \n",
    "    print(f\"✅ Loaded {len(real_df)} real news articles\")\n",
    "    \n",
    "    # Drop rows with empty text\n",
    "    initial_len = len(real_df)\n",
    "    real_df = real_df[real_df['text'].notna() & (real_df['text'].str.strip() != '')].copy()\n",
    "    \n",
    "    if len(real_df) < initial_len:\n",
    "        print(f\"⚠️  Removed {initial_len - len(real_df)} articles with empty text\")\n",
    "    \n",
    "    if auto_label:\n",
    "        print(\"\\nApplying pseudo-labels using FinBERT...\")\n",
    "        \n",
    "        # Initialize FinBERT client\n",
    "        client = FinBERTClient(\n",
    "            model_name=\"yiyanghkust/finbert-tone\",\n",
    "            batch_size=16\n",
    "        )\n",
    "        \n",
    "        # Prepare articles in the format expected by FinBERT client\n",
    "        articles = []\n",
    "        for idx, row in real_df.iterrows():\n",
    "            articles.append({\n",
    "                'id': str(idx),\n",
    "                'text': row['text'],\n",
    "                'metadata': {}\n",
    "            })\n",
    "        \n",
    "        # Run FinBERT analysis\n",
    "        results = client.analyze(articles)\n",
    "        \n",
    "        # Map results back to DataFrame\n",
    "        sentiment_map = {result['id']: result['sentiment'] for result in results}\n",
    "        real_df['label'] = real_df.index.astype(str).map(sentiment_map)\n",
    "        \n",
    "        # Remove any rows where labeling failed\n",
    "        real_df = real_df[real_df['label'].notna()].copy()\n",
    "        \n",
    "        print(f\"✅ Pseudo-labeled {len(real_df)} articles\")\n",
    "        print(f\"   Label distribution:\\n{real_df['label'].value_counts()}\")\n",
    "    \n",
    "    # Keep only text and label columns\n",
    "    real_df = real_df[['text', 'label']].copy()\n",
    "    \n",
    "    return real_df\n",
    "\n",
    "\n",
    "def combine_and_split(\n",
    "    synth_df: pd.DataFrame,\n",
    "    real_df: pd.DataFrame,\n",
    "    data_dir: str = \"./data\",\n",
    "    test_ratio: float = 0.1,\n",
    "    val_ratio: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine synthetic and real datasets, then split into train/val/test.\n",
    "    \n",
    "    Args:\n",
    "        synth_df: Synthetic dataset DataFrame\n",
    "        real_df: Real (pseudo-labeled) dataset DataFrame\n",
    "        data_dir: Directory to save combined datasets\n",
    "        test_ratio: Ratio of data for test set (default: 0.1)\n",
    "        val_ratio: Ratio of remaining data for validation (default: 0.1)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Combining and Splitting Datasets\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Concatenate synthetic and real data\n",
    "    combined_df = pd.concat([synth_df, real_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"Combined dataset size: {len(combined_df)}\")\n",
    "    print(f\"  - Synthetic: {len(synth_df)}\")\n",
    "    print(f\"  - Real: {len(real_df)}\")\n",
    "    print(f\"\\nCombined class distribution:\\n{combined_df['label'].value_counts()}\")\n",
    "    \n",
    "    # Shuffle combined data\n",
    "    combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # First split: test set\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        combined_df,\n",
    "        test_size=test_ratio,\n",
    "        stratify=combined_df['label'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Second split: train and validation from remaining data\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=val_ratio,\n",
    "        stratify=train_val_df['label'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSplit sizes:\")\n",
    "    print(f\"  - Train: {len(train_df)} ({len(train_df)/len(combined_df)*100:.1f}%)\")\n",
    "    print(f\"  - Val:   {len(val_df)} ({len(val_df)/len(combined_df)*100:.1f}%)\")\n",
    "    print(f\"  - Test:  {len(test_df)} ({len(test_df)/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    data_path = Path(data_dir)\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_path = data_path / \"combined_train.csv\"\n",
    "    val_path = data_path / \"combined_val.csv\"\n",
    "    test_path = data_path / \"combined_test.csv\"\n",
    "    \n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    val_df.to_csv(val_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Saved combined datasets:\")\n",
    "    print(f\"   Train: {train_path}\")\n",
    "    print(f\"   Val:   {val_path}\")\n",
    "    print(f\"   Test:  {test_path}\")\n",
    "    \n",
    "    print(f\"\\nTrain class distribution:\\n{train_df['label'].value_counts()}\")\n",
    "    print(f\"\\nVal class distribution:\\n{val_df['label'].value_counts()}\")\n",
    "    print(f\"\\nTest class distribution:\\n{test_df['label'].value_counts()}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"CLI entrypoint for combining datasets.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Combine synthetic and real datasets with pseudo-labeling\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        type=str,\n",
    "        default=\"./data\",\n",
    "        help=\"Directory containing datasets (default: ./data)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--auto_label\",\n",
    "        action=\"store_true\",\n",
    "        default=True,\n",
    "        help=\"Apply pseudo-labels to real data using FinBERT (default: True)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_ratio\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Test set ratio (default: 0.1)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--val_ratio\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Validation set ratio from remaining data (default: 0.1)\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Combined Dataset Preparation\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Data directory: {args.data_dir}\")\n",
    "    print(f\"Auto-label real data: {args.auto_label}\")\n",
    "    print(f\"Test ratio: {args.test_ratio}\")\n",
    "    print(f\"Val ratio: {args.val_ratio}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load synthetic data\n",
    "        synth_df = load_synthetic(data_dir=args.data_dir)\n",
    "        \n",
    "        # Load real data with pseudo-labeling\n",
    "        real_df = load_real(\n",
    "            data_dir=args.data_dir,\n",
    "            auto_label=args.auto_label\n",
    "        )\n",
    "        \n",
    "        # Combine and split\n",
    "        combine_and_split(\n",
    "            synth_df=synth_df,\n",
    "            real_df=real_df,\n",
    "            data_dir=args.data_dir,\n",
    "            test_ratio=args.test_ratio,\n",
    "            val_ratio=args.val_ratio\n",
    "        )\n",
    "        \n",
    "        print(\"\\n✅ Combined dataset preparation complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
