{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfdfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_evaluation.py\n",
    "\"\"\"\n",
    "Evaluate base FinBERT on manually labeled gold test set.\n",
    "Establishes baseline performance before fine-tuning.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "from app.core.finbert_client import FinBERTClient\n",
    "from utils import LABEL2ID, ID2LABEL\n",
    "from config import DATA_DIR\n",
    "\n",
    "\n",
    "def evaluate_gold(test_csv_path: str):\n",
    "    \"\"\"\n",
    "    Evaluate base FinBERT on manually labeled test set.\n",
    "    \n",
    "    Args:\n",
    "        test_csv_path: Path to CSV with columns: text, manual_label\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If test CSV not found\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    test_path = Path(test_csv_path)\n",
    "    if not test_path.exists():\n",
    "        raise FileNotFoundError(f\"Test file not found: {test_csv_path}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Gold Test Set Evaluation - Base FinBERT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Test file: {test_csv_path}\\n\")\n",
    "    \n",
    "    # Load test CSV\n",
    "    print(\"Loading test data...\")\n",
    "    df = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    # Validate columns\n",
    "    if 'text' not in df.columns or 'manual_label' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'text' and 'manual_label' columns\")\n",
    "    \n",
    "    print(f\"✅ Loaded {len(df)} test samples\")\n",
    "    print(f\"\\nManual label distribution:\")\n",
    "    print(df['manual_label'].value_counts())\n",
    "    \n",
    "    # Remove empty texts\n",
    "    initial_len = len(df)\n",
    "    df = df[df['text'].notna() & (df['text'].str.strip() != '')].copy()\n",
    "    \n",
    "    if len(df) < initial_len:\n",
    "        print(f\"⚠️  Removed {initial_len - len(df)} samples with empty text\")\n",
    "    \n",
    "    # Initialize FinBERT client\n",
    "    print(\"\\nInitializing base FinBERT model...\")\n",
    "    client = FinBERTClient(\n",
    "        model_name=\"yiyanghkust/finbert-tone\",\n",
    "        batch_size=16\n",
    "    )\n",
    "    \n",
    "    # Prepare articles for FinBERT client\n",
    "    print(\"\\nRunning FinBERT predictions...\")\n",
    "    articles = []\n",
    "    for idx, row in df.iterrows():\n",
    "        articles.append({\n",
    "            'id': str(idx),\n",
    "            'text': row['text'],\n",
    "            'metadata': {}\n",
    "        })\n",
    "    \n",
    "    # Run FinBERT analysis\n",
    "    results = client.analyze(articles)\n",
    "    \n",
    "    # Map results back to DataFrame\n",
    "    sentiment_map = {result['id']: result['sentiment'] for result in results}\n",
    "    df['finbert_label'] = df.index.astype(str).map(sentiment_map)\n",
    "    \n",
    "    # Remove rows where prediction failed\n",
    "    df = df[df['finbert_label'].notna()].copy()\n",
    "    \n",
    "    print(f\"✅ Generated predictions for {len(df)} samples\")\n",
    "    print(f\"\\nFinBERT label distribution:\")\n",
    "    print(df['finbert_label'].value_counts())\n",
    "    \n",
    "    # Save results with predictions\n",
    "    output_path = test_path.parent / \"real_test_with_predictions.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✅ Saved predictions to: {output_path}\")\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"-\"*70)\n",
    "    print(classification_report(\n",
    "        df['manual_label'],\n",
    "        df['finbert_label'],\n",
    "        target_names=['positive', 'neutral', 'negative'],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"              Predicted\")\n",
    "    print(\"              positive  neutral  negative\")\n",
    "    cm = confusion_matrix(\n",
    "        df['manual_label'],\n",
    "        df['finbert_label'],\n",
    "        labels=['positive', 'neutral', 'negative']\n",
    "    )\n",
    "    for i, label in enumerate(['positive', 'neutral', 'negative']):\n",
    "        print(f\"Actual {label:8s}  {cm[i][0]:8d}  {cm[i][1]:7d}  {cm[i][2]:8d}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Baseline evaluation complete!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"CLI entrypoint for gold test evaluation.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Evaluate base FinBERT on manually labeled test set\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_csv\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to test CSV file (default: DATA_DIR/real_test.csv)\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Use default path if not specified\n",
    "    if args.test_csv is None:\n",
    "        test_csv_path = Path(DATA_DIR) / \"real_test.csv\"\n",
    "    else:\n",
    "        test_csv_path = Path(args.test_csv)\n",
    "    \n",
    "    try:\n",
    "        evaluate_gold(str(test_csv_path))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Evaluation failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
