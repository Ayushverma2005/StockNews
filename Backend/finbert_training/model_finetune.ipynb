{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_finetune.py\n",
    "\"\"\"\n",
    "Fine-tune FinBERT model on combined dataset using HuggingFace Trainer.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(__file__).parent))\n",
    "from utils import set_seed, LABEL2ID, ID2LABEL\n",
    "from config import (\n",
    "    MODEL_NAME,\n",
    "    DATA_DIR,\n",
    "    OUTPUT_DIR,\n",
    "    EPOCHS,\n",
    "    BATCH_SIZE,\n",
    "    LEARNING_RATE,\n",
    "    WEIGHT_DECAY,\n",
    "    WARMUP_STEPS,\n",
    "    EVAL_STEPS,\n",
    "    SAVE_STEPS,\n",
    "    LOGGING_STEPS,\n",
    "    MAX_LENGTH,\n",
    "    SEED\n",
    ")\n",
    "\n",
    "\n",
    "def load_dataset(csv_path: str, tokenizer) -> Dataset:\n",
    "    \"\"\"\n",
    "    Load CSV dataset and tokenize for training.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file with text and label columns\n",
    "        tokenizer: HuggingFace tokenizer instance\n",
    "        \n",
    "    Returns:\n",
    "        HuggingFace Dataset object ready for training\n",
    "    \"\"\"\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Validate columns\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'text' and 'label' columns\")\n",
    "    \n",
    "    # Map labels to IDs\n",
    "    df['label'] = df['label'].map(LABEL2ID)\n",
    "    \n",
    "    # Remove any rows with invalid labels\n",
    "    df = df[df['label'].notna()].copy()\n",
    "    \n",
    "    print(f\"Loaded {len(df)} samples from {csv_path}\")\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    dataset = Dataset.from_pandas(df[['text', 'label']])\n",
    "    \n",
    "    # Apply tokenization\n",
    "    dataset = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    dataset = dataset.rename_column('label', 'labels')\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation during training.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple of (predictions, labels)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metric scores\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate precision, recall, F1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels,\n",
    "        predictions,\n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "\n",
    "def get_trainer(train_dataset, val_dataset, tokenizer, model) -> Trainer:\n",
    "    \"\"\"\n",
    "    Configure and return HuggingFace Trainer.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset: Training dataset\n",
    "        val_dataset: Validation dataset\n",
    "        tokenizer: Tokenizer instance\n",
    "        model: Model instance\n",
    "        \n",
    "    Returns:\n",
    "        Configured Trainer object\n",
    "    \"\"\"\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=SAVE_STEPS,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\"  # Disable wandb/tensorboard\n",
    "    )\n",
    "    \n",
    "    # Create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "\n",
    "def train_and_save():\n",
    "    \"\"\"\n",
    "    Main training function: load data, train model, and save.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"FinBERT Fine-tuning\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(f\"Data directory: {DATA_DIR}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"Epochs: {EPOCHS}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(SEED)\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    print(\"Loading tokenizer and model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=3,\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL2ID\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Model loaded successfully\\n\")\n",
    "    \n",
    "    # Load datasets\n",
    "    data_dir = Path(DATA_DIR)\n",
    "    train_csv = data_dir / \"combined_train.csv\"\n",
    "    val_csv = data_dir / \"combined_val.csv\"\n",
    "    \n",
    "    print(\"Loading and tokenizing datasets...\")\n",
    "    train_dataset = load_dataset(str(train_csv), tokenizer)\n",
    "    val_dataset = load_dataset(str(val_csv), tokenizer)\n",
    "    \n",
    "    print(f\"\\n✅ Train dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"✅ Val dataset: {len(val_dataset)} samples\\n\")\n",
    "    \n",
    "    # Get trainer\n",
    "    print(\"Initializing trainer...\")\n",
    "    trainer = get_trainer(train_dataset, val_dataset, tokenizer, model)\n",
    "    \n",
    "    print(\"✅ Trainer initialized\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"=\"*70)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training complete!\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"Final evaluation on validation set:\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(\"-\"*70)\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    output_path = Path(OUTPUT_DIR)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nSaving fine-tuned model to {OUTPUT_DIR}...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\n✅ Model and tokenizer saved to: {OUTPUT_DIR}\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Fine-tuning complete!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"CLI entrypoint for model fine-tuning.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Fine-tune FinBERT on combined dataset\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        type=str,\n",
    "        default=MODEL_NAME,\n",
    "        help=f\"Base model name (default: {MODEL_NAME})\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        type=str,\n",
    "        default=DATA_DIR,\n",
    "        help=f\"Data directory (default: {DATA_DIR})\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=OUTPUT_DIR,\n",
    "        help=f\"Output directory (default: {OUTPUT_DIR})\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=EPOCHS,\n",
    "        help=f\"Number of epochs (default: {EPOCHS})\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=BATCH_SIZE,\n",
    "        help=f\"Batch size (default: {BATCH_SIZE})\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=LEARNING_RATE,\n",
    "        help=f\"Learning rate (default: {LEARNING_RATE})\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Override config with CLI arguments if provided\n",
    "    if args.model_name != MODEL_NAME:\n",
    "        import config\n",
    "        config.MODEL_NAME = args.model_name\n",
    "    if args.data_dir != DATA_DIR:\n",
    "        import config\n",
    "        config.DATA_DIR = args.data_dir\n",
    "    if args.output_dir != OUTPUT_DIR:\n",
    "        import config\n",
    "        config.OUTPUT_DIR = args.output_dir\n",
    "    if args.epochs != EPOCHS:\n",
    "        import config\n",
    "        config.EPOCHS = args.epochs\n",
    "    if args.batch_size != BATCH_SIZE:\n",
    "        import config\n",
    "        config.BATCH_SIZE = args.batch_size\n",
    "    if args.learning_rate != LEARNING_RATE:\n",
    "        import config\n",
    "        config.LEARNING_RATE = args.learning_rate\n",
    "    \n",
    "    try:\n",
    "        train_and_save()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
